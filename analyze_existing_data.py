#!/usr/bin/env python3
"""
åŸºäºç°æœ‰LiblibAIæ±½è½¦æ¨¡å‹æ•°æ®çš„æ·±åº¦åˆ†æ
ä¸ºè®¾è®¡å¸ˆæä¾›è¶‹åŠ¿æ´å¯Ÿå’Œè®¾è®¡å»ºè®®
"""

import json
import os
from typing import Dict, List, Any, Tuple
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datetime import datetime
import jieba
import re
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ExistingDataAnalyzer:
    def __init__(self, data_file: str = 'liblib_car_models_analysis.json'):
        self.data_file = data_file
        self.models_data = []
        self.summary_data = {}
        self.output_dir = 'existing_data_analysis'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        self.load_data()
        
    def load_data(self):
        """åŠ è½½ç°æœ‰æ¨¡å‹æ•°æ®"""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.summary_data = data.get('summary', {})
                self.models_data = data.get('models', [])
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.models_data)} ä¸ªæ±½è½¦æ¨¡å‹æ•°æ®")
            print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
            print(f"   æ€»æµè§ˆé‡: {self.summary_data.get('total_views', 0):,}")
            print(f"   æ€»ç‚¹èµæ•°: {self.summary_data.get('total_likes', 0):,}")
            print(f"   æ€»ä¸‹è½½æ•°: {self.summary_data.get('total_downloads', 0):,}")
            
        except FileNotFoundError:
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            return
        except json.JSONDecodeError:
            print(f"âŒ æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯: {self.data_file}")
            return
    
    def analyze_model_types_and_performance(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ç±»å‹å’Œæ€§èƒ½è¡¨ç°"""
        print("ğŸ¯ åˆ†ææ¨¡å‹ç±»å‹å’Œæ€§èƒ½è¡¨ç°...")
        
        # æ¨¡å‹ç±»å‹ç»Ÿè®¡
        model_types = Counter()
        base_models = Counter()
        type_performance = defaultdict(lambda: {
            'views': [], 'likes': [], 'downloads': [], 'models': []
        })
        
        for model in self.models_data:
            model_type = model.get('type', 'Unknown')
            base_model = model.get('baseModel', 'Unknown')
            
            model_types[model_type] += 1
            base_models[base_model] += 1
            
            # è½¬æ¢å­—ç¬¦ä¸²æ•°å€¼
            views = self.parse_number(model.get('views', '0'))
            likes = self.parse_number(model.get('likes', '0'))
            downloads = self.parse_number(model.get('downloads', '0'))
            
            type_performance[model_type]['views'].append(views)
            type_performance[model_type]['likes'].append(likes)
            type_performance[model_type]['downloads'].append(downloads)
            type_performance[model_type]['models'].append(model)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_analysis = {}
        for model_type, data in type_performance.items():
            if data['views']:
                performance_analysis[model_type] = {
                    'count': len(data['views']),
                    'avg_views': np.mean(data['views']),
                    'avg_likes': np.mean(data['likes']),
                    'avg_downloads': np.mean(data['downloads']),
                    'engagement_rate': np.mean(data['likes']) / max(np.mean(data['views']), 1) * 100,
                    'download_rate': np.mean(data['downloads']) / max(np.mean(data['views']), 1) * 100,
                    'top_model': max(data['models'], key=lambda x: self.parse_number(x.get('views', '0')))
                }
        
        # åˆ›å»ºå¯è§†åŒ–
        self.plot_model_performance(performance_analysis)
        
        return {
            'model_types': dict(model_types),
            'base_models': dict(base_models),
            'performance_analysis': performance_analysis
        }
    
    def analyze_content_and_keywords(self) -> Dict[str, Any]:
        """åˆ†æå†…å®¹å…³é”®è¯å’Œè®¾è®¡è¶‹åŠ¿"""
        print("ğŸ” åˆ†æå†…å®¹å…³é”®è¯å’Œè®¾è®¡è¶‹åŠ¿...")
        
        all_keywords = []
        title_keywords = []
        car_style_keywords = {
            'è®¾è®¡é£æ ¼': [],
            'è½¦è¾†ç±»å‹': [],
            'æ¸²æŸ“é£æ ¼': [],
            'æŠ€æœ¯ç‰¹å¾': []
        }
        
        # å…³é”®è¯åˆ†ç±»è¯å…¸
        style_categories = {
            'è®¾è®¡é£æ ¼': ['ç§‘å¹»', 'ç°ä»£', 'å¤å¤', 'è±ªå', 'è¿åŠ¨', 'æç®€', 'å·¥ä¸š', 'æœªæ¥', 'ç»å…¸', 'æ¦‚å¿µ'],
            'è½¦è¾†ç±»å‹': ['è·‘è½¦', 'è½¿è½¦', 'SUV', 'å¡è½¦', 'å·´å£«', 'æ‘©æ‰˜', 'èµ›è½¦', 'çš®å¡', 'æ¦‚å¿µè½¦', 'F1'],
            'æ¸²æŸ“é£æ ¼': ['å†™å®', 'æ’ç”»', '3D', 'æ¸²æŸ“', 'æ‰‹ç»˜', 'ç…§ç‰‡', 'å»ºæ¨¡', 'è´¨æ„Ÿ', 'å…‰å½±'],
            'æŠ€æœ¯ç‰¹å¾': ['AI', 'ç”Ÿæˆ', 'æ¨¡å‹', 'LoRA', 'Checkpoint', 'è®­ç»ƒ', 'ç®—æ³•', 'å‚æ•°']
        }
        
        keyword_performance = defaultdict(lambda: {
            'count': 0, 'total_views': 0, 'total_likes': 0, 'models': []
        })
        
        for model in self.models_data:
            title = model.get('title', '')
            description = model.get('description', '')
            text = f"{title} {description}".lower()
            
            views = self.parse_number(model.get('views', '0'))
            likes = self.parse_number(model.get('likes', '0'))
            
            # æå–ä¸­æ–‡å…³é”®è¯
            chinese_words = re.findall(r'[\u4e00-\u9fff]+', title)
            title_keywords.extend(chinese_words)
            all_keywords.extend(chinese_words)
            
            # æå–è‹±æ–‡å…³é”®è¯
            english_words = re.findall(r'[A-Za-z]+', title)
            title_keywords.extend(english_words)
            all_keywords.extend(english_words)
            
            # åˆ†ç±»å…³é”®è¯
            for category, keywords in style_categories.items():
                found_keywords = []
                for keyword in keywords:
                    if keyword.lower() in text or keyword in title:
                        found_keywords.append(keyword)
                        car_style_keywords[category].append(keyword)
                        
                        # ç»Ÿè®¡å…³é”®è¯æ€§èƒ½
                        keyword_performance[keyword]['count'] += 1
                        keyword_performance[keyword]['total_views'] += views
                        keyword_performance[keyword]['total_likes'] += likes
                        keyword_performance[keyword]['models'].append(model)
        
        # åˆ†æå…³é”®è¯è¶‹åŠ¿
        keyword_trends = {}
        for keyword, data in keyword_performance.items():
            if data['count'] >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                keyword_trends[keyword] = {
                    'count': data['count'],
                    'avg_views': data['total_views'] / data['count'],
                    'avg_likes': data['total_likes'] / data['count'],
                    'engagement_score': (data['total_likes'] * 1.0 + data['total_views'] * 0.1) / data['count']
                }
        
        # ç”Ÿæˆè¯äº‘æ•°æ®
        title_word_counts = Counter(title_keywords)
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºCounterå¯¹è±¡ä»¥ä¾›åç»­ä½¿ç”¨
        style_counters_for_viz = {}
        for k, v in car_style_keywords.items():
            style_counters_for_viz[k] = Counter(v) if v else Counter()
        self.generate_keyword_analysis(title_word_counts, style_counters_for_viz)
        
        # ç¡®ä¿å°†åˆ—è¡¨è½¬æ¢ä¸ºCounterå¯¹è±¡
        style_counters = {}
        for k, v in car_style_keywords.items():
            style_counters[k] = Counter(v) if v else Counter()
        
        return {
            'keyword_frequencies': dict(Counter(all_keywords).most_common(20)),
            'style_categories': style_counters,
            'keyword_trends': keyword_trends,
            'content_insights': self.generate_content_insights(style_counters, keyword_trends)
        }
    
    def analyze_author_strategies(self) -> Dict[str, Any]:
        """åˆ†æä½œè€…ç­–ç•¥å’ŒæˆåŠŸæ¨¡å¼"""
        print("ğŸ‘¥ åˆ†æä½œè€…ç­–ç•¥å’ŒæˆåŠŸæ¨¡å¼...")
        
        author_stats = defaultdict(lambda: {
            'models': [], 'total_views': 0, 'total_likes': 0, 'total_downloads': 0,
            'specialties': [], 'avg_performance': {}
        })
        
        for model in self.models_data:
            author = model.get('author', 'Unknown')
            views = self.parse_number(model.get('views', '0'))
            likes = self.parse_number(model.get('likes', '0'))
            downloads = self.parse_number(model.get('downloads', '0'))
            
            author_stats[author]['models'].append(model)
            author_stats[author]['total_views'] += views
            author_stats[author]['total_likes'] += likes
            author_stats[author]['total_downloads'] += downloads
            
            # åˆ†æä¸“é•¿
            title = model.get('title', '').lower()
            if any(word in title for word in ['æ¦‚å¿µ', 'concept']):
                author_stats[author]['specialties'].append('æ¦‚å¿µè®¾è®¡')
            if any(word in title for word in ['æ¸²æŸ“', 'render']):
                author_stats[author]['specialties'].append('æ¸²æŸ“æŠ€æœ¯')
            if any(word in title for word in ['è´¨æ„Ÿ', 'æè´¨']):
                author_stats[author]['specialties'].append('æè´¨è¡¨ç°')
        
        # è®¡ç®—ä½œè€…æˆåŠŸæŒ‡æ ‡
        author_rankings = []
        for author, data in author_stats.items():
            model_count = len(data['models'])
            if model_count >= 1:  # è‡³å°‘æœ‰1ä¸ªæ¨¡å‹
                engagement_score = (
                    data['total_likes'] * 2.0 + 
                    data['total_downloads'] * 3.0 + 
                    data['total_views'] * 0.1
                ) / model_count
                
                author_rankings.append({
                    'author': author,
                    'model_count': model_count,
                    'avg_views': data['total_views'] / model_count,
                    'avg_likes': data['total_likes'] / model_count,
                    'avg_downloads': data['total_downloads'] / model_count,
                    'engagement_score': engagement_score,
                    'main_specialties': list(set(data['specialties']))[:3],
                    'best_model': max(data['models'], key=lambda x: self.parse_number(x.get('views', '0')))
                })
        
        author_rankings.sort(key=lambda x: x['engagement_score'], reverse=True)
        
        return {
            'total_authors': len(author_stats),
            'top_authors': author_rankings[:10],
            'author_insights': self.generate_author_insights(author_rankings)
        }
    
    def analyze_market_trends(self) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºè¶‹åŠ¿å’Œæœºä¼š"""
        print("ğŸ“ˆ åˆ†æå¸‚åœºè¶‹åŠ¿å’Œæœºä¼š...")
        
        # æŒ‰æ€§èƒ½æ’åºæ¨¡å‹
        sorted_models = sorted(
            self.models_data,
            key=lambda x: (
                self.parse_number(x.get('likes', '0')) * 2 + 
                self.parse_number(x.get('downloads', '0')) * 3 +
                self.parse_number(x.get('views', '0')) * 0.1
            ),
            reverse=True
        )
        
        # åˆ†æçƒ­é—¨æ¨¡å‹ç‰¹å¾
        top_models = sorted_models[:6]  # å‰6ä¸ªæ¨¡å‹
        trending_features = {
            'model_types': Counter(),
            'design_themes': Counter(),
            'naming_patterns': []
        }
        
        for model in top_models:
            model_type = model.get('type', '')
            title = model.get('title', '')
            
            trending_features['model_types'][model_type] += 1
            trending_features['naming_patterns'].append(title)
            
            # æå–è®¾è®¡ä¸»é¢˜
            if any(word in title.lower() for word in ['ç§‘å¹»', 'sci-fi', 'æœªæ¥']):
                trending_features['design_themes']['ç§‘å¹»é£æ ¼'] += 1
            if any(word in title.lower() for word in ['æ¦‚å¿µ', 'concept']):
                trending_features['design_themes']['æ¦‚å¿µè®¾è®¡'] += 1
            if any(word in title.lower() for word in ['è´¨æ„Ÿ', 'æè´¨', 'æ¸²æŸ“']):
                trending_features['design_themes']['è´¨æ„Ÿè¡¨ç°'] += 1
            if any(word in title.lower() for word in ['è±ªå', 'luxury']):
                trending_features['design_themes']['è±ªåé£æ ¼'] += 1
        
        # è¯†åˆ«å¸‚åœºæœºä¼š
        market_opportunities = self.identify_opportunities(sorted_models)
        
        # ç”Ÿæˆè¶‹åŠ¿é¢„æµ‹
        trend_predictions = self.predict_trends(trending_features, sorted_models)
        
        return {
            'top_performing_models': [
                {
                    'title': model.get('title', ''),
                    'author': model.get('author', ''),
                    'type': model.get('type', ''),
                    'views': model.get('views', ''),
                    'likes': model.get('likes', ''),
                    'downloads': model.get('downloads', '')
                }
                for model in top_models
            ],
            'trending_features': {
                'model_types': dict(trending_features['model_types']),
                'design_themes': dict(trending_features['design_themes']),
                'naming_patterns': trending_features['naming_patterns']
            },
            'market_opportunities': market_opportunities,
            'trend_predictions': trend_predictions
        }
    
    def parse_number(self, value: str) -> int:
        """è§£ææ•°å­—å­—ç¬¦ä¸²"""
        if isinstance(value, (int, float)):
            return int(value)
        
        if isinstance(value, str):
            # ç§»é™¤é€—å·å’Œç©ºæ ¼
            clean_value = value.replace(',', '').replace(' ', '')
            try:
                return int(clean_value)
            except ValueError:
                return 0
        
        return 0
    
    def generate_content_insights(self, style_categories: Dict, keyword_trends: Dict) -> List[str]:
        """ç”Ÿæˆå†…å®¹æ´å¯Ÿ"""
        insights = []
        
        # åˆ†æè®¾è®¡é£æ ¼è¶‹åŠ¿
        design_styles = style_categories.get('è®¾è®¡é£æ ¼', Counter())
        if design_styles:
            top_style = design_styles.most_common(1)[0]
            insights.append(f"ğŸ¨ {top_style[0]} æ˜¯å½“å‰æœ€å—æ¬¢è¿çš„è®¾è®¡é£æ ¼ï¼Œå‡ºç° {top_style[1]} æ¬¡")
        
        # åˆ†æè½¦è¾†ç±»å‹è¶‹åŠ¿
        vehicle_types = style_categories.get('è½¦è¾†ç±»å‹', Counter())
        if vehicle_types:
            top_vehicle = vehicle_types.most_common(1)[0]
            insights.append(f"ğŸš— {top_vehicle[0]} æ˜¯æœ€çƒ­é—¨çš„è½¦è¾†ç±»å‹ï¼Œæœ‰ {top_vehicle[1]} ä¸ªç›¸å…³æ¨¡å‹")
        
        # åˆ†æé«˜ä»·å€¼å…³é”®è¯
        if keyword_trends:
            high_value_keywords = [
                (k, v['engagement_score']) 
                for k, v in keyword_trends.items() 
                if v['engagement_score'] > 1000
            ]
            if high_value_keywords:
                best_keyword = max(high_value_keywords, key=lambda x: x[1])
                insights.append(f"â­ '{best_keyword[0]}' æ˜¯æœ€æœ‰ä»·å€¼çš„å…³é”®è¯ï¼Œå‚ä¸åº¦å¾—åˆ†: {best_keyword[1]:.0f}")
        
        return insights
    
    def generate_author_insights(self, author_rankings: List) -> List[str]:
        """ç”Ÿæˆä½œè€…æ´å¯Ÿ"""
        insights = []
        
        if author_rankings:
            top_author = author_rankings[0]
            insights.append(f"ğŸ‘‘ {top_author['author']} æ˜¯è¡¨ç°æœ€ä½³çš„ä½œè€…ï¼Œå¹³å‡å‚ä¸åº¦å¾—åˆ†: {top_author['engagement_score']:.0f}")
            
            # åˆ†æå¤šäº§ä½œè€…
            productive_authors = [a for a in author_rankings if a['model_count'] >= 2]
            if productive_authors:
                insights.append(f"ğŸ­ {len(productive_authors)} ä½ä½œè€…å‘å¸ƒäº†å¤šä¸ªæ¨¡å‹ï¼Œæ˜¾ç¤ºæŒç»­åˆ›ä½œèƒ½åŠ›")
            
            # åˆ†æä¸“ä¸šåŒ–ç¨‹åº¦
            specialized_authors = [a for a in author_rankings if len(a['main_specialties']) >= 2]
            if specialized_authors:
                insights.append(f"ğŸ¯ {len(specialized_authors)} ä½ä½œè€…å±•ç°å‡ºæ˜ç¡®çš„ä¸“ä¸šåŒ–æ–¹å‘")
        
        return insights
    
    def identify_opportunities(self, sorted_models: List) -> List[Dict[str, str]]:
        """è¯†åˆ«å¸‚åœºæœºä¼š"""
        opportunities = []
        
        # åˆ†ææ¨¡å‹ç±»å‹åˆ†å¸ƒ
        model_types = Counter(model.get('type', '') for model in self.models_data)
        underrepresented_types = [t for t, count in model_types.items() if count <= 2 and t]
        
        if underrepresented_types:
            opportunities.append({
                'type': 'æ¨¡å‹ç±»å‹æœºä¼š',
                'description': f"å…³æ³¨ {', '.join(underrepresented_types[:3])} ç­‰ç±»å‹",
                'reason': 'è¿™äº›æ¨¡å‹ç±»å‹ç«äº‰è¾ƒå°‘ï¼Œæœ‰å‘å±•ç©ºé—´'
            })
        
        # åˆ†ææˆåŠŸæ¨¡å¼
        top_models = sorted_models[:5]
        common_features = []
        
        for model in top_models:
            title = model.get('title', '').lower()
            if 'f.1' in title or 'f1' in title:
                common_features.append('F.1ç³»åˆ—')
            if any(word in title for word in ['è´¨æ„Ÿ', 'æ¸²æŸ“', 'å…‰å½±']):
                common_features.append('è´¨æ„Ÿè¡¨ç°')
            if any(word in title for word in ['æ¦‚å¿µ', 'concept']):
                common_features.append('æ¦‚å¿µè®¾è®¡')
        
        feature_counts = Counter(common_features)
        if feature_counts:
            top_feature = feature_counts.most_common(1)[0]
            opportunities.append({
                'type': 'æˆåŠŸæ¨¡å¼',
                'description': f"é‡ç‚¹å‘å±• {top_feature[0]} ç›¸å…³å†…å®¹",
                'reason': f'åœ¨å‰5åæ¨¡å‹ä¸­å‡ºç° {top_feature[1]} æ¬¡ï¼Œè¯æ˜å—æ¬¢è¿åº¦é«˜'
            })
        
        return opportunities
    
    def predict_trends(self, trending_features: Dict, sorted_models: List) -> List[Dict[str, str]]:
        """é¢„æµ‹è¶‹åŠ¿"""
        predictions = []
        
        # åŸºäºçƒ­é—¨æ¨¡å‹ç±»å‹é¢„æµ‹
        top_model_types = trending_features.get('model_types', Counter())
        if top_model_types:
            dominant_type = top_model_types.most_common(1)[0]
            predictions.append({
                'trend': 'æ¨¡å‹æŠ€æœ¯å‘å±•',
                'prediction': f"{dominant_type[0]} å°†ç»§ç»­ä¸»å¯¼å¸‚åœº",
                'confidence': 'é«˜',
                'basis': f'åœ¨é¡¶çº§æ¨¡å‹ä¸­å æ¯” {dominant_type[1]/6*100:.1f}%'
            })
        
        # åŸºäºè®¾è®¡ä¸»é¢˜é¢„æµ‹
        design_themes = trending_features.get('design_themes', Counter())
        if design_themes:
            emerging_theme = design_themes.most_common(1)[0]
            predictions.append({
                'trend': 'è®¾è®¡é£æ ¼è¶‹åŠ¿',
                'prediction': f"{emerging_theme[0]} å°†æˆä¸ºä¸»æµè®¾è®¡æ–¹å‘",
                'confidence': 'ä¸­ç­‰',
                'basis': f'åœ¨çƒ­é—¨æ¨¡å‹ä¸­é¢‘ç¹å‡ºç°'
            })
        
        # åŸºäºæ€§èƒ½æ•°æ®é¢„æµ‹
        avg_engagement = np.mean([
            self.parse_number(model.get('likes', '0')) + self.parse_number(model.get('downloads', '0'))
            for model in sorted_models[:10]
        ])
        
        predictions.append({
            'trend': 'ç”¨æˆ·å‚ä¸åº¦',
            'prediction': 'é«˜è´¨é‡æ¨¡å‹çš„ç”¨æˆ·å‚ä¸åº¦å°†æŒç»­æå‡',
            'confidence': 'é«˜',
            'basis': f'é¡¶çº§æ¨¡å‹å¹³å‡å‚ä¸åº¦è¾¾åˆ° {avg_engagement:.0f}'
        })
        
        return predictions
    
    def plot_model_performance(self, performance_analysis: Dict):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½åˆ†æå›¾"""
        if not performance_analysis:
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('LiblibAI æ±½è½¦æ¨¡å‹æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
        
        types = list(performance_analysis.keys())
        
        # 1. å¹³å‡æµè§ˆé‡
        avg_views = [performance_analysis[t]['avg_views'] for t in types]
        ax1.bar(types, avg_views, color=sns.color_palette("viridis", len(types)))
        ax1.set_title('å„ç±»å‹æ¨¡å‹å¹³å‡æµè§ˆé‡')
        ax1.set_ylabel('å¹³å‡æµè§ˆé‡')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. å¹³å‡ç‚¹èµæ•°
        avg_likes = [performance_analysis[t]['avg_likes'] for t in types]
        ax2.bar(types, avg_likes, color=sns.color_palette("plasma", len(types)))
        ax2.set_title('å„ç±»å‹æ¨¡å‹å¹³å‡ç‚¹èµæ•°')
        ax2.set_ylabel('å¹³å‡ç‚¹èµæ•°')
        ax2.tick_params(axis='x', rotation=45)
        
        # 3. å‚ä¸åº¦å¯¹æ¯”
        engagement_rates = [performance_analysis[t]['engagement_rate'] for t in types]
        download_rates = [performance_analysis[t]['download_rate'] for t in types]
        
        x = np.arange(len(types))
        width = 0.35
        
        ax3.bar(x - width/2, engagement_rates, width, label='ç‚¹èµç‡(%)', color='skyblue')
        ax3.bar(x + width/2, download_rates, width, label='ä¸‹è½½ç‡(%)', color='orange')
        ax3.set_title('ç”¨æˆ·å‚ä¸åº¦å¯¹æ¯”')
        ax3.set_ylabel('æ¯”ç‡ (%)')
        ax3.set_xticks(x)
        ax3.set_xticklabels(types, rotation=45)
        ax3.legend()
        
        # 4. æ¨¡å‹æ•°é‡åˆ†å¸ƒ
        model_counts = [performance_analysis[t]['count'] for t in types]
        ax4.pie(model_counts, labels=types, autopct='%1.1f%%', startangle=90)
        ax4.set_title('æ¨¡å‹ç±»å‹åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'model_performance_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜: {self.output_dir}/model_performance_analysis.png")
    
    def generate_keyword_analysis(self, word_counts: Counter, style_categories: Dict):
        """ç”Ÿæˆå…³é”®è¯åˆ†æå›¾è¡¨"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('æ±½è½¦æ¨¡å‹å…³é”®è¯å’Œé£æ ¼åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. çƒ­é—¨å…³é”®è¯
        top_words = word_counts.most_common(10)
        if top_words:
            words, counts = zip(*top_words)
            ax1.barh(words, counts, color=sns.color_palette("Set2", len(words)))
            ax1.set_title('çƒ­é—¨å…³é”®è¯ Top 10')
            ax1.set_xlabel('å‡ºç°é¢‘æ¬¡')
        
        # 2. è®¾è®¡é£æ ¼åˆ†å¸ƒ
        design_styles = style_categories.get('è®¾è®¡é£æ ¼', Counter())
        if design_styles:
            styles, style_counts = zip(*design_styles.most_common())
            ax2.pie(style_counts, labels=styles, autopct='%1.1f%%', startangle=90)
            ax2.set_title('è®¾è®¡é£æ ¼åˆ†å¸ƒ')
        
        # 3. è½¦è¾†ç±»å‹åˆ†å¸ƒ
        vehicle_types = style_categories.get('è½¦è¾†ç±»å‹', Counter())
        if vehicle_types:
            vehicles, vehicle_counts = zip(*vehicle_types.most_common())
            ax3.bar(vehicles, vehicle_counts, color=sns.color_palette("husl", len(vehicles)))
            ax3.set_title('è½¦è¾†ç±»å‹åˆ†å¸ƒ')
            ax3.set_ylabel('å‡ºç°é¢‘æ¬¡')
            ax3.tick_params(axis='x', rotation=45)
        
        # 4. æŠ€æœ¯ç‰¹å¾
        tech_features = style_categories.get('æŠ€æœ¯ç‰¹å¾', Counter())
        if tech_features:
            features, feature_counts = zip(*tech_features.most_common())
            ax4.barh(features, feature_counts, color=sns.color_palette("coolwarm", len(features)))
            ax4.set_title('æŠ€æœ¯ç‰¹å¾åˆ†å¸ƒ')
            ax4.set_xlabel('å‡ºç°é¢‘æ¬¡')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'keyword_style_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ” å…³é”®è¯åˆ†æå›¾è¡¨å·²ä¿å­˜: {self.output_dir}/keyword_style_analysis.png")
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ç”ŸæˆLiblibAIæ±½è½¦æ¨¡å‹æ·±åº¦åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        model_analysis = self.analyze_model_types_and_performance()
        content_analysis = self.analyze_content_and_keywords()
        author_analysis = self.analyze_author_strategies()
        market_analysis = self.analyze_market_trends()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'data_source': self.data_file,
                'total_models': len(self.models_data),
                'analysis_scope': 'LiblibAI æ±½è½¦äº¤é€šæ¿å—ç°æœ‰æ¨¡å‹'
            },
            
            'executive_summary': {
                'total_models': len(self.models_data),
                'total_views': self.summary_data.get('total_views', 0),
                'total_likes': self.summary_data.get('total_likes', 0),
                'total_downloads': self.summary_data.get('total_downloads', 0),
                'avg_performance': {
                    'views': self.summary_data.get('avg_views', 0),
                    'likes': self.summary_data.get('avg_likes', 0),
                    'downloads': self.summary_data.get('avg_downloads', 0)
                },
                'top_model_types': list(model_analysis['model_types'].keys())[:3],
                'leading_authors': [author['author'] for author in author_analysis['top_authors'][:5]]
            },
            
            'detailed_analysis': {
                'model_performance': model_analysis,
                'content_keywords': content_analysis,
                'author_strategies': author_analysis,
                'market_trends': market_analysis
            },
            
            'design_recommendations': self.generate_design_recommendations(
                model_analysis, content_analysis, market_analysis
            ),
            
            'strategic_insights': self.generate_strategic_insights(
                model_analysis, author_analysis, market_analysis, content_analysis
            )
        }
        
        return comprehensive_report
    
    def generate_design_recommendations(self, model_analysis, content_analysis, market_analysis) -> List[Dict]:
        """ç”Ÿæˆè®¾è®¡å»ºè®®"""
        recommendations = []
        
        # åŸºäºè¡¨ç°æœ€ä½³çš„æ¨¡å‹ç±»å‹
        performance = model_analysis.get('performance_analysis', {})
        if performance:
            best_type = max(performance.items(), key=lambda x: x[1]['engagement_rate'])[0]
            recommendations.append({
                'category': 'æ¨¡å‹ç±»å‹é€‰æ‹©',
                'title': f'é‡ç‚¹å‘å±• {best_type} ç±»å‹æ¨¡å‹',
                'description': f'è¯¥ç±»å‹å…·æœ‰æœ€é«˜çš„ç”¨æˆ·å‚ä¸åº¦ ({performance[best_type]["engagement_rate"]:.2f}%)',
                'priority': 'é«˜',
                'expected_impact': 'æå‡ç”¨æˆ·äº’åŠ¨å’Œä¸‹è½½ç‡'
            })
        
        # åŸºäºçƒ­é—¨å…³é”®è¯
        keyword_trends = content_analysis.get('keyword_trends', {})
        if keyword_trends:
            top_keyword = max(keyword_trends.items(), key=lambda x: x[1]['engagement_score'])[0]
            recommendations.append({
                'category': 'å†…å®¹ä¼˜åŒ–',
                'title': f'åœ¨ä½œå“ä¸­çªå‡º "{top_keyword}" å…ƒç´ ',
                'description': f'è¯¥å…³é”®è¯å…·æœ‰æœ€é«˜çš„å‚ä¸åº¦å¾—åˆ† ({keyword_trends[top_keyword]["engagement_score"]:.0f})',
                'priority': 'ä¸­',
                'expected_impact': 'æé«˜ä½œå“æ›å…‰åº¦å’Œç”¨æˆ·å‘ç°ç‡'
            })
        
        # åŸºäºå¸‚åœºæœºä¼š
        opportunities = market_analysis.get('market_opportunities', [])
        for opp in opportunities[:2]:
            recommendations.append({
                'category': 'å¸‚åœºæœºä¼š',
                'title': opp.get('description', ''),
                'description': opp.get('reason', ''),
                'priority': 'ä¸­',
                'expected_impact': 'å¼€æ‹“æ–°çš„å¸‚åœºç©ºé—´'
            })
        
        return recommendations
    
    def generate_strategic_insights(self, model_analysis, author_analysis, market_analysis, content_analysis=None) -> List[Dict]:
        """ç”Ÿæˆæˆ˜ç•¥æ´å¯Ÿ"""
        insights = []
        
        # å¸‚åœºé›†ä¸­åº¦åˆ†æ
        top_authors = author_analysis.get('top_authors', [])
        if len(top_authors) >= 3:
            top_3_share = sum(author['model_count'] for author in top_authors[:3])
            total_models = len(self.models_data)
            concentration = (top_3_share / total_models) * 100
            
            insights.append({
                'type': 'å¸‚åœºç»“æ„',
                'title': f'å¸‚åœºé›†ä¸­åº¦: {concentration:.1f}%',
                'description': f'å‰3åä½œè€…è´¡çŒ®äº† {concentration:.1f}% çš„æ¨¡å‹',
                'implication': 'å¸‚åœºä»æœ‰è¾ƒå¤§ç©ºé—´ç»™æ–°è¿›å…¥è€…' if concentration < 50 else 'å¸‚åœºç›¸å¯¹é›†ä¸­ï¼Œéœ€è¦å·®å¼‚åŒ–ç­–ç•¥'
            })
        
        # æŠ€æœ¯è¶‹åŠ¿åˆ†æ
        model_types = model_analysis.get('model_types', {})
        if 'LORAF.1' in model_types:
            lora_share = (model_types['LORAF.1'] / sum(model_types.values())) * 100
            insights.append({
                'type': 'æŠ€æœ¯è¶‹åŠ¿',
                'title': f'LoRA F.1 æŠ€æœ¯å ä¸»å¯¼åœ°ä½ ({lora_share:.1f}%)',
                'description': 'LoRA F.1 æ˜¯å½“å‰æœ€å—æ¬¢è¿çš„æ¨¡å‹æŠ€æœ¯',
                'implication': 'å»ºè®®æ·±å…¥å­¦ä¹ å’Œä¼˜åŒ– LoRA F.1 æŠ€æœ¯æ ˆ'
            })
        
        # å†…å®¹ç­–ç•¥åˆ†æ
        if content_analysis:
            content_insights = content_analysis.get('content_insights', [])
            if content_insights:
                insights.append({
                    'type': 'å†…å®¹ç­–ç•¥',
                    'title': 'è®¾è®¡é£æ ¼å¤šå…ƒåŒ–æœºä¼š',
                    'description': '; '.join(content_insights[:2]),
                    'implication': 'åœ¨çƒ­é—¨é£æ ¼åŸºç¡€ä¸Šæ¢ç´¢åˆ›æ–°è¡¨è¾¾æ–¹å¼'
                })
        
        return insights
    
    def save_report(self, report: Dict[str, Any]):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = os.path.join(self.output_dir, 'comprehensive_analysis_report.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(report)
        
        print(f"âœ… å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° {self.output_dir}/")
    
    def generate_markdown_report(self, report: Dict[str, Any]):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        md_content = f"""# LiblibAI æ±½è½¦è®¾è®¡æ¨¡å‹æ·±åº¦åˆ†ææŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {report['report_metadata']['generated_at']}  
**æ•°æ®æ¥æº**: {report['report_metadata']['data_source']}  
**åˆ†ææ¨¡å‹æ•°é‡**: {report['report_metadata']['total_models']}

### ğŸ¯ æ ¸å¿ƒæ•°æ®

- **æ€»æµè§ˆé‡**: {report['executive_summary']['total_views']:,}
- **æ€»ç‚¹èµæ•°**: {report['executive_summary']['total_likes']:,}  
- **æ€»ä¸‹è½½æ•°**: {report['executive_summary']['total_downloads']:,}
- **å¹³å‡æ€§èƒ½**: æµè§ˆ {report['executive_summary']['avg_performance']['views']:.0f} | ç‚¹èµ {report['executive_summary']['avg_performance']['likes']:.0f} | ä¸‹è½½ {report['executive_summary']['avg_performance']['downloads']:.0f}

### ğŸ† å¸‚åœºé¢†å¯¼è€…

**é¡¶çº§æ¨¡å‹ç±»å‹**: {', '.join(report['executive_summary']['top_model_types'])}  
**leadingä½œè€…**: {', '.join(report['executive_summary']['leading_authors'])}

## ğŸ“ˆ è¯¦ç»†åˆ†æ

### æ¨¡å‹ç±»å‹æ€§èƒ½åˆ†æ

{self._format_model_performance(report['detailed_analysis']['model_performance'])}

### å†…å®¹å…³é”®è¯è¶‹åŠ¿

{self._format_content_analysis(report['detailed_analysis']['content_keywords'])}

### ä½œè€…ç­–ç•¥åˆ†æ

{self._format_author_analysis(report['detailed_analysis']['author_strategies'])}

### å¸‚åœºè¶‹åŠ¿é¢„æµ‹

{self._format_market_trends(report['detailed_analysis']['market_trends'])}

## ğŸ’¡ è®¾è®¡å»ºè®®

{self._format_design_recommendations(report['design_recommendations'])}

## ğŸ¯ æˆ˜ç•¥æ´å¯Ÿ

{self._format_strategic_insights(report['strategic_insights'])}

## ğŸ“ åˆ†æå›¾è¡¨

- `model_performance_analysis.png`: æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ
- `keyword_style_analysis.png`: å…³é”®è¯å’Œé£æ ¼åˆ†å¸ƒ
- `comprehensive_analysis_report.json`: å®Œæ•´æ•°æ®æŠ¥å‘Š

---
*æœ¬æŠ¥å‘ŠåŸºäºLiblibAIå¹³å°ç°æœ‰æ±½è½¦äº¤é€šæ¨¡å‹æ•°æ®ç”Ÿæˆ*
"""
        
        md_file = os.path.join(self.output_dir, 'analysis_report.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def _format_model_performance(self, performance_data: Dict) -> str:
        """æ ¼å¼åŒ–æ¨¡å‹æ€§èƒ½æ•°æ®"""
        lines = []
        model_types = performance_data.get('model_types', {})
        performance_analysis = performance_data.get('performance_analysis', {})
        
        lines.append("#### æ¨¡å‹ç±»å‹åˆ†å¸ƒ")
        for model_type, count in model_types.items():
            if model_type in performance_analysis:
                perf = performance_analysis[model_type]
                lines.append(f"- **{model_type}**: {count} ä¸ªæ¨¡å‹ï¼Œå¹³å‡æµè§ˆé‡ {perf['avg_views']:.0f}ï¼Œå‚ä¸åº¦ {perf['engagement_rate']:.2f}%")
        
        return '\n'.join(lines)
    
    def _format_content_analysis(self, content_data: Dict) -> str:
        """æ ¼å¼åŒ–å†…å®¹åˆ†ææ•°æ®"""
        lines = []
        
        lines.append("#### çƒ­é—¨å…³é”®è¯")
        keyword_freq = content_data.get('keyword_frequencies', {})
        for keyword, freq in list(keyword_freq.items())[:5]:
            lines.append(f"- **{keyword}**: {freq} æ¬¡")
        
        lines.append("\n#### è®¾è®¡é£æ ¼è¶‹åŠ¿")
        style_categories = content_data.get('style_categories', {})
        for category, styles in style_categories.items():
            if styles:
                top_style = styles.most_common(1)[0]
                lines.append(f"- **{category}**: {top_style[0]} æœ€å—æ¬¢è¿ ({top_style[1]} æ¬¡)")
        
        return '\n'.join(lines)
    
    def _format_author_analysis(self, author_data: Dict) -> str:
        """æ ¼å¼åŒ–ä½œè€…åˆ†ææ•°æ®"""
        lines = []
        top_authors = author_data.get('top_authors', [])
        
        lines.append("#### é¡¶çº§ä½œè€…æ’è¡Œ")
        for i, author in enumerate(top_authors[:5], 1):
            lines.append(f"{i}. **{author['author']}**: {author['model_count']} ä¸ªæ¨¡å‹ï¼Œå‚ä¸åº¦å¾—åˆ† {author['engagement_score']:.0f}")
        
        return '\n'.join(lines)
    
    def _format_market_trends(self, market_data: Dict) -> str:
        """æ ¼å¼åŒ–å¸‚åœºè¶‹åŠ¿æ•°æ®"""
        lines = []
        
        lines.append("#### è¡¨ç°æœ€ä½³æ¨¡å‹")
        top_models = market_data.get('top_performing_models', [])
        for i, model in enumerate(top_models[:3], 1):
            lines.append(f"{i}. **{model['title']}** by {model['author']} - æµè§ˆé‡: {model['views']}")
        
        lines.append("\n#### è¶‹åŠ¿é¢„æµ‹")
        predictions = market_data.get('trend_predictions', [])
        for pred in predictions[:3]:
            lines.append(f"- **{pred['trend']}**: {pred['prediction']} (ç½®ä¿¡åº¦: {pred['confidence']})")
        
        return '\n'.join(lines)
    
    def _format_design_recommendations(self, recommendations: List) -> str:
        """æ ¼å¼åŒ–è®¾è®¡å»ºè®®"""
        lines = []
        for i, rec in enumerate(recommendations, 1):
            lines.append(f"### {i}. {rec['title']}")
            lines.append(f"**ç±»åˆ«**: {rec['category']}")
            lines.append(f"**æè¿°**: {rec['description']}")
            lines.append(f"**ä¼˜å…ˆçº§**: {rec['priority']}")
            lines.append(f"**é¢„æœŸå½±å“**: {rec['expected_impact']}")
            lines.append("")
        return '\n'.join(lines)
    
    def _format_strategic_insights(self, insights: List) -> str:
        """æ ¼å¼åŒ–æˆ˜ç•¥æ´å¯Ÿ"""
        lines = []
        for insight in insights:
            lines.append(f"### {insight['title']}")
            lines.append(f"**ç±»å‹**: {insight['type']}")
            lines.append(f"**æè¿°**: {insight['description']}")
            lines.append(f"**å¯ç¤º**: {insight['implication']}")
            lines.append("")
        return '\n'.join(lines)

def main():
    """ä¸»å‡½æ•°"""
    analyzer = ExistingDataAnalyzer()
    
    if not analyzer.models_data:
        print("âŒ æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
        return
    
    try:
        # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        report = analyzer.generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        analyzer.save_report(report)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š åˆ†æäº† {len(analyzer.models_data)} ä¸ªæ±½è½¦æ¨¡å‹")
        print(f"ğŸ’¡ ç”Ÿæˆäº† {len(report['design_recommendations'])} æ¡è®¾è®¡å»ºè®®")
        print(f"ğŸ¯ æä¾›äº† {len(report['strategic_insights'])} æ¡æˆ˜ç•¥æ´å¯Ÿ")
        print(f"ğŸ“ æŠ¥å‘Šä¿å­˜åœ¨ {analyzer.output_dir}/ ç›®å½•ä¸­")
        
        # æ‰“å°å…³é”®å‘ç°
        print(f"\nğŸ”¥ å…³é”®å‘ç°:")
        print(f"   æœ€å—æ¬¢è¿æ¨¡å‹ç±»å‹: {list(report['detailed_analysis']['model_performance']['model_types'].keys())[0]}")
        print(f"   é¡¶çº§ä½œè€…: {report['executive_summary']['leading_authors'][0]}")
        print(f"   å¹³å‡ç”¨æˆ·å‚ä¸åº¦: {report['executive_summary']['avg_performance']['likes']:.0f} ç‚¹èµ")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
