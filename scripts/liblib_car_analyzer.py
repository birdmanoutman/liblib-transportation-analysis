#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Liblib æ±½è½¦äº¤é€šæ¨¡å‹å®Œæ•´åˆ†æå™¨
æ•´åˆæ•°æ®é‡‡é›†ã€å›¾ç‰‡ä¸‹è½½ã€æ•°æ®åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„ç»Ÿä¸€å·¥å…·

åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸš— æ™ºèƒ½æ•°æ®é‡‡é›†ï¼ˆAPI + æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼‰
- ğŸ–¼ï¸ æ‰¹é‡å›¾ç‰‡ä¸‹è½½ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
- ğŸ“Š æ·±åº¦æ•°æ®åˆ†æï¼ˆå¤šç»´åº¦ç»Ÿè®¡ï¼‰
- ğŸ“ ä¸“ä¸šæŠ¥å‘Šç”Ÿæˆï¼ˆMarkdown + å›¾è¡¨ï¼‰
- âš¡ é«˜æ€§èƒ½å¹¶å‘å¤„ç†
- ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python liblib_car_analyzer.py [é€‰é¡¹]

é€‰é¡¹ï¼š
    --collect    æ‰§è¡Œæ•°æ®é‡‡é›†
    --download  æ‰§è¡Œå›¾ç‰‡ä¸‹è½½
    --analyze   æ‰§è¡Œæ•°æ®åˆ†æ
    --report    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    --all       æ‰§è¡Œå®Œæ•´æµç¨‹
    --config    æŒ‡å®šé…ç½®æ–‡ä»¶
    --output    æŒ‡å®šè¾“å‡ºç›®å½•
    --help      æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹ï¼š
    python liblib_car_analyzer.py --all
    python liblib_car_analyzer.py --collect --download
    python liblib_car_analyzer.py --config config.json
"""

import os
import sys
import json
import time
import logging
import argparse
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import requests
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from urllib.parse import urlparse, urljoin
import re

# å¯¼å…¥é…ç½®ç®¡ç†æ¨¡å—
try:
    from config_manager import ConfigManager
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®ç®¡ç†å™¨
    class ConfigManager:
        def __init__(self):
            self.config_data = {}
        def get(self, key_path, default=None):
            return default
        def load_config(self):
            return {}

# å°è¯•å¯¼å…¥Playwrightï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwrightæœªå®‰è£…ï¼Œæµè§ˆå™¨è‡ªåŠ¨åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install playwright && playwright install")

class LiblibCarModelsAnalyzer:
    """Liblibæ±½è½¦äº¤é€šæ¨¡å‹å®Œæ•´åˆ†æå™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager()
        
        # åŠ è½½é…ç½®
        if config:
            self.config_manager.config_data = config
        else:
            self.config_manager.load_config()
        
        self.config = self.config_manager.get_effective_config()
        self._setup_logging()
        self._setup_directories()
        self._setup_session()
        
        # æ•°æ®å­˜å‚¨
        self.models_data = []
        self.collected_models = set()
        self.analysis_results = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_models': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'start_time': time.time()
        }
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.config_manager.get_effective_config()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # è·å–æ—¥å¿—é…ç½®
        log_level = getattr(logging, self.config_manager.get('logging.level', 'INFO').upper(), logging.INFO)
        file_logging = self.config_manager.get('logging.file_logging', True)
        console_logging = self.config_manager.get('logging.console_logging', True)
        
        # è®¾ç½®æ—¥å¿—æ ¼å¼
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
        logging.getLogger().handlers.clear()
        
        # è®¾ç½®æ ¹æ—¥å¿—å™¨çº§åˆ«
        logging.getLogger().setLevel(log_level)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(log_format)
        
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        if file_logging:
            log_dir = Path(self.config['storage']['output_dir']) / self.config['storage']['logs_dir']
            log_dir.mkdir(parents=True, exist_ok=True)
            log_file = log_dir / f"liblib_analyzer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            logging.getLogger().addHandler(file_handler)
        
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
        if console_logging:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            logging.getLogger().addHandler(console_handler)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("Liblibæ±½è½¦äº¤é€šæ¨¡å‹åˆ†æå™¨å¯åŠ¨")
    
    def _setup_directories(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_dir = Path(self.config['storage']['output_dir'])
        self.images_dir = self.output_dir / self.config['storage']['images_dir']
        self.data_dir = self.output_dir / self.config['storage']['data_dir']
        self.reports_dir = self.output_dir / self.config['storage']['reports_dir']
        
        for dir_path in [self.output_dir, self.images_dir, self.data_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
    
    def _setup_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.liblib.art/',
            'Origin': 'https://www.liblib.art'
        })
        
        # è®¾ç½®è¶…æ—¶å’Œé‡è¯•é…ç½®
        self.timeout = self.config_manager.get('api.timeout', 30)
        self.retry_times = self.config_manager.get('api.retry_times', 3)
        self.retry_delay = self.config_manager.get('api.retry_delay', 2)
    
    def safe_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(self.retry_times):
            try:
                response = self.session.request(
                    method, url, 
                    timeout=self.timeout,
                    **kwargs
                )
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_times}) {url}: {e}")
                if attempt < self.retry_times - 1:
                    time.sleep(self.retry_delay ** attempt)
        return None
    
    def get_timestamp(self) -> int:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return int(time.time() * 1000)
    
    async def collect_data_api(self) -> List[Dict]:
        """é€šè¿‡APIé‡‡é›†æ•°æ®"""
        self.logger.info("å¼€å§‹APIæ•°æ®é‡‡é›†...")
        all_models = []
        
        max_pages = self.config_manager.get('scraping.max_pages', 10)
        delay_between_pages = self.config_manager.get('scraping.delay_between_pages', 1)
        
        for page in range(1, max_pages + 1):
            models = self._get_models_by_page(page)
            if not models:
                self.logger.info(f"ç¬¬{page}é¡µæ— æ•°æ®ï¼Œåœæ­¢é‡‡é›†")
                break
            
            all_models.extend(models)
            self.logger.info(f"ç¬¬{page}é¡µé‡‡é›†åˆ°{len(models)}ä¸ªæ¨¡å‹")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if page < max_pages:
                time.sleep(delay_between_pages)
        
        self.logger.info(f"APIé‡‡é›†å®Œæˆï¼Œå…±è·å–{len(all_models)}ä¸ªæ¨¡å‹")
        return all_models
    
    def _get_models_by_page(self, page: int) -> List[Dict]:
        """è·å–æŒ‡å®šé¡µçš„æ¨¡å‹æ•°æ®"""
        url = f"{self.config['api_base']}/api/www/model/list"
        
        payload = {
            "categories": ["æ±½è½¦äº¤é€š"],
            "page": page,
            "pageSize": self.config['page_size'],
            "sortType": "recommend",
            "modelType": "",
            "nsfw": False
        }
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                if data.get('data', {}).get('list'):
                    return data['data']['list']
            except json.JSONDecodeError:
                self.logger.error("å“åº”JSONè§£æå¤±è´¥")
        
        return []
    
    async def collect_data_browser(self) -> List[Dict]:
        """é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–é‡‡é›†æ•°æ®"""
        if not PLAYWRIGHT_AVAILABLE:
            self.logger.warning("Playwrightä¸å¯ç”¨ï¼Œè·³è¿‡æµè§ˆå™¨é‡‡é›†")
            return []
        
        self.logger.info("å¼€å§‹æµè§ˆå™¨æ•°æ®é‡‡é›†...")
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # è®¿é—®é¡µé¢
                await page.goto(f"{self.config['base_url']}/models?category=æ±½è½¦äº¤é€š")
                await page.wait_for_load_state('networkidle')
                
                # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                models = await self._scroll_and_extract(page)
                
                await browser.close()
                self.logger.info(f"æµè§ˆå™¨é‡‡é›†å®Œæˆï¼Œå…±è·å–{len(models)}ä¸ªæ¨¡å‹")
                return models
                
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨é‡‡é›†å¤±è´¥: {e}")
            return []
    
    async def _scroll_and_extract(self, page) -> List[Dict]:
        """æ»šåŠ¨é¡µé¢å¹¶æå–æ•°æ®"""
        models = []
        last_height = await page.evaluate("document.body.scrollHeight")
        
        while len(models) < 200:  # æœ€å¤§200ä¸ªæ¨¡å‹
            # æ»šåŠ¨åˆ°åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(2000)
            
            # æå–å½“å‰é¡µé¢çš„æ¨¡å‹æ•°æ®
            new_models = await page.evaluate("""
                () => {
                    const models = [];
                    const cards = document.querySelectorAll('div[role="gridcell"]');
                    
                    cards.forEach(card => {
                        try {
                            const link = card.querySelector('a');
                            if (!link || !link.href) return;
                            
                            const modelId = link.href.match(/modelinfo\/([^/?]+)/)?.[1];
                            if (!modelId) return;
                            
                            const title = card.querySelector('h6')?.textContent?.trim() || '';
                            const author = card.querySelector('div:last-child div:last-child')?.textContent?.trim() || '';
                            const imageUrl = card.querySelector('img')?.src || '';
                            
                            if (title && author) {
                                models.push({
                                    id: modelId,
                                    title: title,
                                    author: author,
                                    imageUrl: imageUrl,
                                    url: link.href
                                });
                            }
                        } catch (e) {}
                    });
                    
                    return models;
                }
            """)
            
            # å»é‡
            for model in new_models:
                if model['id'] not in [m['id'] for m in models]:
                    models.append(model)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨
            new_height = await page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
            
            self.logger.info(f"å·²é‡‡é›†{len(models)}ä¸ªæ¨¡å‹ï¼Œç»§ç»­æ»šåŠ¨...")
        
        return models
    
    async def collect_data_enhanced(self) -> List[Dict]:
        """å¢å¼ºæœç´¢ç­–ç•¥é‡‡é›†æ•°æ®"""
        self.logger.info("å¼€å§‹å¢å¼ºæœç´¢æ•°æ®é‡‡é›†...")
        all_models = []
        
        # é€šè¿‡å…³é”®è¯æœç´¢
        for keyword in self.config['car_keywords'][:10]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            models = self._search_models_by_keyword(keyword)
            all_models.extend(models)
            self.logger.info(f"å…³é”®è¯'{keyword}'æœç´¢åˆ°{len(models)}ä¸ªæ¨¡å‹")
            time.sleep(1)
        
        # å»é‡
        unique_models = []
        seen_ids = set()
        for model in all_models:
            if model.get('id') not in seen_ids:
                unique_models.append(model)
                seen_ids.add(model.get('id'))
        
        self.logger.info(f"å¢å¼ºæœç´¢å®Œæˆï¼Œå…±è·å–{len(unique_models)}ä¸ªå”¯ä¸€æ¨¡å‹")
        return unique_models
    
    def _search_models_by_keyword(self, keyword: str) -> List[Dict]:
        """é€šè¿‡å…³é”®è¯æœç´¢æ¨¡å‹"""
        url = f"{self.config['api_base']}/api/www/model/list"
        
        payload = {
            "keyword": keyword,
            "page": 1,
            "pageSize": 24,
            "sortType": "recommend",
            "modelType": "",
            "nsfw": False
        }
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                if data.get('data', {}).get('list'):
                    return data['data']['list']
            except json.JSONDecodeError:
                self.logger.error("æœç´¢å“åº”JSONè§£æå¤±è´¥")
        
        return []
    
    async def collect_all_data(self) -> List[Dict]:
        """é‡‡é›†æ‰€æœ‰æ•°æ®ï¼ˆå¤šç§ç­–ç•¥ç»“åˆï¼‰"""
        self.logger.info("å¼€å§‹ç»¼åˆæ•°æ®é‡‡é›†...")
        
        # 1. APIé‡‡é›†ï¼ˆä¸»è¦æ–¹å¼ï¼‰
        api_models = await self.collect_data_api()
        
        # 2. æµè§ˆå™¨é‡‡é›†ï¼ˆè¡¥å……ï¼‰
        browser_models = await self.collect_data_browser()
        
        # 3. å¢å¼ºæœç´¢ï¼ˆè¡¥å……ï¼‰
        enhanced_models = await self.collect_data_enhanced()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_models = []
        seen_ids = set()
        
        for model_list in [api_models, browser_models, enhanced_models]:
            for model in model_list:
                model_id = model.get('id') or model.get('uuid')
                if model_id and model_id not in seen_ids:
                    all_models.append(model)
                    seen_ids.add(model_id)
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        detailed_models = []
        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            future_to_model = {
                executor.submit(self._get_model_detail, model): model 
                for model in all_models[:50]  # é™åˆ¶æ•°é‡
            }
            
            for future in as_completed(future_to_model):
                model = future_to_model[future]
                try:
                    detail = future.result()
                    if detail:
                        detailed_models.append(detail)
                except Exception as e:
                    self.logger.error(f"è·å–æ¨¡å‹è¯¦æƒ…å¤±è´¥: {e}")
        
        self.logger.info(f"ç»¼åˆé‡‡é›†å®Œæˆï¼Œå…±è·å–{len(detailed_models)}ä¸ªè¯¦ç»†æ¨¡å‹")
        return detailed_models
    
    def _get_model_detail(self, model: Dict) -> Optional[Dict]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        model_id = model.get('id') or model.get('uuid')
        if not model_id:
            return None
        
        url = f"{self.config['api_base']}/api/www/model/getByUuid/{model_id}"
        params = {"timestamp": self.get_timestamp()}
        
        response = self.safe_request('POST', url, params=params)
        if response:
            try:
                data = response.json()
                if data.get('data'):
                    return data['data']
            except json.JSONDecodeError:
                self.logger.error("æ¨¡å‹è¯¦æƒ…JSONè§£æå¤±è´¥")
        
        return None
    
    async def download_images(self, models: List[Dict]) -> Dict:
        """æ‰¹é‡ä¸‹è½½å›¾ç‰‡"""
        self.logger.info("å¼€å§‹æ‰¹é‡å›¾ç‰‡ä¸‹è½½...")
        
        download_results = {
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total': len(models)
        }
        
        def download_single_image(model):
            """ä¸‹è½½å•ä¸ªå›¾ç‰‡"""
            try:
                image_url = model.get('coverUrl') or model.get('imageUrl')
                if not image_url:
                    return 'skipped'
                
                # ç”Ÿæˆæ–‡ä»¶å
                title = model.get('title', 'unknown')
                safe_title = re.sub(r'[^\w\s-]', '', title)[:50]
                filename = f"{safe_title}_{model.get('id', 'unknown')}"
                
                # è·å–æ–‡ä»¶æ‰©å±•å
                parsed_url = urlparse(image_url)
                path = parsed_url.path
                if '.' in path:
                    ext = path.split('.')[-1]
                    if ext.lower() in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                        filename = f"{filename}.{ext}"
                    else:
                        filename = f"{filename}.jpg"
                else:
                    filename = f"{filename}.jpg"
                
                filepath = self.images_dir / filename
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if filepath.exists():
                    return 'skipped'
                
                # ä¸‹è½½å›¾ç‰‡
                response = self.safe_request('GET', image_url)
                if response:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    return 'success'
                else:
                    return 'failed'
                    
            except Exception as e:
                self.logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
                return 'failed'
        
        # å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            future_to_model = {
                executor.submit(download_single_image, model): model 
                for model in models
            }
            
            for future in as_completed(future_to_model):
                result = future.result()
                if result == 'success':
                    download_results['successful'] += 1
                elif result == 'failed':
                    download_results['failed'] += 1
                else:
                    download_results['skipped'] += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                total_processed = download_results['successful'] + download_results['failed'] + download_results['skipped']
                if total_processed % 5 == 0:
                    self.logger.info(f"ä¸‹è½½è¿›åº¦: {total_processed}/{download_results['total']}")
        
        self.logger.info(f"å›¾ç‰‡ä¸‹è½½å®Œæˆ: æˆåŠŸ{download_results['successful']}, å¤±è´¥{download_results['failed']}, è·³è¿‡{download_results['skipped']}")
        return download_results
    
    def analyze_data(self, models: List[Dict]) -> Dict:
        """åˆ†ææ•°æ®"""
        self.logger.info("å¼€å§‹æ•°æ®åˆ†æ...")
        
        if not models:
            self.logger.warning("æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return {}
        
        # æ•°æ®é¢„å¤„ç†
        df = pd.DataFrame(models)
        
        # è§£ææ•°å€¼å­—æ®µ
        numeric_fields = ['views', 'likes', 'downloads']
        for field in numeric_fields:
            if field in df.columns:
                df[field] = df[field].apply(self._parse_number)
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = {
            'total_models': len(models),
            'unique_authors': df['author'].nunique() if 'author' in df.columns else 0,
            'model_types': df['type'].value_counts().to_dict() if 'type' in df.columns else {},
            'total_views': df['views'].sum() if 'views' in df.columns else 0,
            'total_likes': df['likes'].sum() if 'likes' in df.columns else 0,
            'total_downloads': df['downloads'].sum() if 'downloads' in df.columns else 0
        }
        
        # è®¡ç®—å¹³å‡å€¼
        if 'views' in df.columns:
            basic_stats['avg_views'] = df['views'].mean()
        if 'likes' in df.columns:
            basic_stats['avg_likes'] = df['likes'].mean()
        if 'downloads' in df.columns:
            basic_stats['avg_downloads'] = df['downloads'].mean()
        
        # ä½œè€…åˆ†æ
        author_stats = {}
        if 'author' in df.columns and 'views' in df.columns:
            author_stats = df.groupby('author').agg({
                'views': 'sum',
                'likes': 'sum',
                'downloads': 'sum'
            }).sort_values('views', ascending=False).head(10).to_dict('index')
        
        # æ¨¡å‹ç±»å‹åˆ†æ
        type_stats = {}
        if 'type' in df.columns and 'views' in df.columns:
            type_stats = df.groupby('type').agg({
                'views': 'sum',
                'likes': 'sum',
                'downloads': 'sum',
                'id': 'count'
            }).rename(columns={'id': 'count'}).to_dict('index')
        
        # å‚ä¸åº¦åˆ†æ
        engagement_stats = {}
        if 'views' in df.columns and 'likes' in df.columns and 'downloads' in df.columns:
            df['engagement_rate'] = (df['likes'] + df['downloads']) / df['views']
            engagement_stats = {
                'avg_engagement_rate': df['engagement_rate'].mean(),
                'top_engagement_models': df.nlargest(5, 'engagement_rate')[['title', 'author', 'engagement_rate']].to_dict('records')
            }
        
        analysis_results = {
            'basic_stats': basic_stats,
            'author_stats': author_stats,
            'type_stats': type_stats,
            'engagement_stats': engagement_stats,
            'raw_data': models
        }
        
        self.logger.info("æ•°æ®åˆ†æå®Œæˆ")
        return analysis_results
    
    def _parse_number(self, value) -> Union[int, float]:
        """è§£ææ•°å­—å­—ç¬¦ä¸²"""
        if pd.isna(value) or value is None:
            return 0
        
        if isinstance(value, (int, float)):
            return value
        
        if isinstance(value, str):
            # å¤„ç†k, wç­‰åç¼€
            value = value.lower().strip()
            if 'k' in value:
                return float(value.replace('k', '')) * 1000
            elif 'w' in value:
                return float(value.replace('w', '')) * 10000
            elif value.isdigit():
                return int(value)
            else:
                # å°è¯•æå–æ•°å­—
                numbers = re.findall(r'\d+\.?\d*', value)
                if numbers:
                    return float(numbers[0])
        
        return 0
    
    def generate_report(self, analysis_results: Dict) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        self.logger.info("å¼€å§‹ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        if not analysis_results:
            return "æ— æ•°æ®å¯ç”ŸæˆæŠ¥å‘Š"
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = self._generate_markdown_report(analysis_results)
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.reports_dir / f"liblib_car_analysis_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)
    
    def _generate_markdown_report(self, analysis_results: Dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        basic_stats = analysis_results.get('basic_stats', {})
        author_stats = analysis_results.get('author_stats', {})
        type_stats = analysis_results.get('type_stats', {})
        engagement_stats = analysis_results.get('engagement_stats', {})
        
        report = f"""# Liblib æ±½è½¦äº¤é€šæ¨¡å‹åˆ†ææŠ¥å‘Š

## ğŸ“Š åŸºç¡€ç»Ÿè®¡

- **æ€»æ¨¡å‹æ•°é‡**: {basic_stats.get('total_models', 0)}
- **å”¯ä¸€ä½œè€…æ•°**: {basic_stats.get('unique_authors', 0)}
- **æ€»æµè§ˆé‡**: {basic_stats.get('total_views', 0):,}
- **æ€»ç‚¹èµæ•°**: {basic_stats.get('total_likes', 0):,}
- **æ€»ä¸‹è½½é‡**: {basic_stats.get('total_downloads', 0):,}
- **å¹³å‡æµè§ˆé‡**: {basic_stats.get('avg_views', 0):,.1f}
- **å¹³å‡ç‚¹èµæ•°**: {basic_stats.get('avg_likes', 0):,.1f}
- **å¹³å‡ä¸‹è½½é‡**: {basic_stats.get('avg_downloads', 0):,.1f}

## ğŸ† æ¨¡å‹ç±»å‹åˆ†å¸ƒ

"""
        
        for model_type, stats in type_stats.items():
            report += f"- **{model_type}**: {stats.get('count', 0)}ä¸ªæ¨¡å‹\n"
        
        report += "\n## ğŸ‘¥ ä½œè€…æ’è¡Œæ¦œ (Top 10)\n\n"
        
        for i, (author, stats) in enumerate(author_stats.items(), 1):
            report += f"{i}. **{author}**\n"
            report += f"   - æµè§ˆé‡: {stats.get('views', 0):,}\n"
            report += f"   - ç‚¹èµæ•°: {stats.get('likes', 0):,}\n"
            report += f"   - ä¸‹è½½é‡: {stats.get('downloads', 0):,}\n\n"
        
        if engagement_stats:
            report += f"## ğŸ“ˆ å‚ä¸åº¦åˆ†æ\n\n"
            report += f"- **å¹³å‡å‚ä¸ç‡**: {engagement_stats.get('avg_engagement_rate', 0):.2%}\n\n"
            report += f"- **å‚ä¸åº¦æœ€é«˜çš„æ¨¡å‹**:\n"
            for model in engagement_stats.get('top_engagement_models', []):
                report += f"  - {model.get('title', 'Unknown')} (ä½œè€…: {model.get('author', 'Unknown')})\n"
                report += f"    å‚ä¸ç‡: {model.get('engagement_rate', 0):.2%}\n\n"
        
        report += f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        report += f"*æ•°æ®æ¥æº: Liblib.art æ±½è½¦äº¤é€šæ¿å—*\n"
        
        return report
    
    def save_data(self, models: List[Dict], analysis_results: Dict):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        self.logger.info("ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜åŸå§‹æ•°æ®
        data_file = self.data_dir / f"models_data_{timestamp}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(models, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.data_dir / f"analysis_results_{timestamp}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜: {data_file}, {analysis_file}")
    
    async def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        self.logger.info("å¼€å§‹è¿è¡Œå®Œæ•´åˆ†ææµç¨‹...")
        
        try:
            # 1. æ•°æ®é‡‡é›†
            self.logger.info("=== ç¬¬ä¸€é˜¶æ®µ: æ•°æ®é‡‡é›† ===")
            models = await self.collect_all_data()
            if not models:
                self.logger.error("æ•°æ®é‡‡é›†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                return False
            
            # 2. å›¾ç‰‡ä¸‹è½½
            self.logger.info("=== ç¬¬äºŒé˜¶æ®µ: å›¾ç‰‡ä¸‹è½½ ===")
            download_results = await self.download_images(models)
            
            # 3. æ•°æ®åˆ†æ
            self.logger.info("=== ç¬¬ä¸‰é˜¶æ®µ: æ•°æ®åˆ†æ ===")
            analysis_results = self.analyze_data(models)
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            self.logger.info("=== ç¬¬å››é˜¶æ®µ: ç”ŸæˆæŠ¥å‘Š ===")
            report_file = self.generate_report(analysis_results)
            
            # 5. ä¿å­˜æ•°æ®
            self.logger.info("=== ç¬¬äº”é˜¶æ®µ: ä¿å­˜æ•°æ® ===")
            self.save_data(models, analysis_results)
            
            # 6. ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = time.time() - self.stats['start_time']
            self.logger.info(f"=== åˆ†æå®Œæˆ ===")
            self.logger.info(f"æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            self.logger.info(f"é‡‡é›†æ¨¡å‹: {len(models)}ä¸ª")
            self.logger.info(f"å›¾ç‰‡ä¸‹è½½: æˆåŠŸ{download_results['successful']}ä¸ª")
            self.logger.info(f"æŠ¥å‘Šæ–‡ä»¶: {report_file}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ†ææµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Liblib æ±½è½¦äº¤é€šæ¨¡å‹å®Œæ•´åˆ†æå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # åŸºæœ¬åŠŸèƒ½å‚æ•°
    parser.add_argument('--collect', action='store_true', help='æ‰§è¡Œæ•°æ®é‡‡é›†')
    parser.add_argument('--download', action='store_true', help='æ‰§è¡Œå›¾ç‰‡ä¸‹è½½')
    parser.add_argument('--analyze', action='store_true', help='æ‰§è¡Œæ•°æ®åˆ†æ')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--all', action='store_true', help='æ‰§è¡Œå®Œæ•´æµç¨‹')
    
    # é…ç½®ç›¸å…³å‚æ•°
    parser.add_argument('--config', type=str, help='æŒ‡å®šé…ç½®æ–‡ä»¶')
    parser.add_argument('--create-config', action='store_true', help='åˆ›å»ºé…ç½®æ¨¡æ¿æ–‡ä»¶')
    parser.add_argument('--show-config', action='store_true', help='æ˜¾ç¤ºå½“å‰é…ç½®æ‘˜è¦')
    
    # T10å·¥å•è¦æ±‚çš„å‚æ•°åŒ–é…ç½®
    # æ ‡ç­¾ç›¸å…³
    parser.add_argument('--tags', type=str, help='æŒ‡å®šè¦é‡‡é›†çš„æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--exclude-tags', type=str, help='æŒ‡å®šè¦æ’é™¤çš„æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--custom-keywords', type=str, help='è‡ªå®šä¹‰å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”')
    
    # æ’åºç›¸å…³
    parser.add_argument('--sort-by', type=str, choices=['downloads', 'likes', 'created_at', 'updated_at', 'name'], 
                       help='æŒ‡å®šæ’åºå­—æ®µ')
    parser.add_argument('--sort-order', type=str, choices=['asc', 'desc'], help='æŒ‡å®šæ’åºé¡ºåº')
    
    # é¡µèŒƒå›´ç›¸å…³
    parser.add_argument('--max-pages', type=int, help='æœ€å¤§é‡‡é›†é¡µæ•°')
    parser.add_argument('--page-size', type=int, help='æ¯é¡µæ¨¡å‹æ•°é‡')
    
    # å¹¶å‘ç›¸å…³
    parser.add_argument('--max-workers', type=int, help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--concurrent-downloads', type=int, help='å¹¶å‘ä¸‹è½½æ•°é‡')
    
    # å­˜å‚¨è·¯å¾„ç›¸å…³
    parser.add_argument('--output-dir', type=str, help='æŒ‡å®šè¾“å‡ºç›®å½•')
    parser.add_argument('--images-dir', type=str, help='æŒ‡å®šå›¾ç‰‡å­˜å‚¨ç›®å½•')
    
    # æ—¥å¿—ç›¸å…³
    parser.add_argument('--log-level', type=str, choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       help='æŒ‡å®šæ—¥å¿—çº§åˆ«')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—è¾“å‡º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = ConfigManager()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        config_manager.load_config(args.config)
    else:
        config_manager.load_config()
    
    # ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
    config_manager.update_from_args(args)
    
    # éªŒè¯é…ç½®
    validation_errors = config_manager.validate_config()
    if validation_errors:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
        for error in validation_errors:
            print(f"  - {error}")
        return
    
    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    if args.show_config:
        config_manager.print_config_summary()
        return
    
    # åˆ›å»ºé…ç½®æ¨¡æ¿
    if args.create_config:
        if config_manager.create_config_template():
            print("âœ… é…ç½®æ¨¡æ¿åˆ›å»ºæˆåŠŸ")
        else:
            print("âŒ é…ç½®æ¨¡æ¿åˆ›å»ºå¤±è´¥")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = LiblibCarModelsAnalyzer(config_manager.get_effective_config())
    
    try:
        if args.all or (not any([args.collect, args.download, args.analyze, args.report])):
            # è¿è¡Œå®Œæ•´æµç¨‹
            asyncio.run(analyzer.run_full_analysis())
        else:
            # è¿è¡ŒæŒ‡å®šåŠŸèƒ½
            if args.collect:
                print("æ•°æ®é‡‡é›†åŠŸèƒ½éœ€è¦å®Œæ•´æµç¨‹æ”¯æŒ")
            if args.download:
                print("å›¾ç‰‡ä¸‹è½½åŠŸèƒ½éœ€è¦å®Œæ•´æµç¨‹æ”¯æŒ")
            if args.analyze:
                print("æ•°æ®åˆ†æåŠŸèƒ½éœ€è¦å®Œæ•´æµç¨‹æ”¯æŒ")
            if args.report:
                print("æŠ¥å‘Šç”ŸæˆåŠŸèƒ½éœ€è¦å®Œæ•´æµç¨‹æ”¯æŒ")
            print("å»ºè®®ä½¿ç”¨ --all å‚æ•°è¿è¡Œå®Œæ•´æµç¨‹")
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        analyzer.logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
