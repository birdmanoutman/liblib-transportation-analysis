#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T5 å¢å¼ºç‰ˆè¯¦æƒ…é‡‡é›†å™¨
é›†æˆé…ç½®ç®¡ç†ã€æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œé™é€Ÿæ§åˆ¶
"""

import os
import sys
import json
import time
import logging
import mysql.connector
from mysql.connector import Error
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from urllib.parse import urlparse
import re
import threading
from queue import Queue
import signal

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# å¯¼å…¥é…ç½®
try:
    from detail_collector_config import CollectorConfig, load_config
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    class CollectorConfig:
        def __init__(self):
            load_dotenv()
            self.db_host = os.getenv('DB_HOST', 'localhost')
            self.db_port = int(os.getenv('DB_PORT', '3306'))
            self.db_name = os.getenv('DB_NAME', 'cardesignspace')
            self.db_user = os.getenv('DB_USER', 'root')
            self.db_password = os.getenv('DB_PASSWORD', '')
            self.max_workers = 5
            self.requests_per_second = 4.0
            self.collect_comments = True
            self.collect_author_info = True
            self.strict_validation = False
            self.log_level = 'INFO'
    
    def load_config():
        return CollectorConfig()

class RateLimiter:
    """è¯·æ±‚é™é€Ÿå™¨"""
    
    def __init__(self, requests_per_second: float):
        self.requests_per_second = requests_per_second
        self.delay = 1.0 / requests_per_second
        self.last_request_time = 0
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """ç­‰å¾…å¦‚æœéœ€è¦é™é€Ÿ"""
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            
            if time_since_last < self.delay:
                sleep_time = self.delay - time_since_last
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()

class EnhancedDetailCollector:
    """T5 å¢å¼ºç‰ˆè¯¦æƒ…é‡‡é›†å™¨"""
    
    def __init__(self, config: CollectorConfig = None):
        # åŠ è½½é…ç½®
        self.config = config or load_config()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–é™é€Ÿå™¨
        self.rate_limiter = RateLimiter(self.config.requests_per_second)
        
        # æ•°æ®åº“è¿æ¥é…ç½®
        self.db_config = {
            'host': self.config.db_host,
            'port': self.config.db_port,
            'database': self.config.db_name,
            'user': self.config.db_user,
            'password': self.config.db_password,
            'charset': 'utf8mb4',
            'collation': 'utf8mb4_unicode_ci'
        }
        
        # APIé…ç½®
        self.api_base = self.config.api_base
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.liblib.art/',
            'Origin': 'https://www.liblib.art'
        })
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'failed_count': 0,
            'authors_created': 0,
            'works_created': 0,
            'comments_created': 0,
            'start_time': datetime.now(),
            'errors': []
        }
        
        # æ•°æ®åº“è¿æ¥
        self.connection = None
        self.connect_database()
        
        # ä¿¡å·å¤„ç†
        self.setup_signal_handlers()
        
        # åŸå§‹æ•°æ®ä¿å­˜
        if self.config.save_raw_data:
            os.makedirs(self.config.raw_data_dir, exist_ok=True)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config.log_level.upper(), logging.INFO)
        
        logging.basicConfig(
            level=log_level,
            format=self.config.log_format,
            handlers=[
                logging.FileHandler(self.config.log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            self.logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
            self.close()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def connect_database(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.connection = mysql.connector.connect(**self.db_config)
            if self.connection.is_connected():
                self.logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            else:
                self.logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
                raise Exception("æ•°æ®åº“è¿æ¥å¤±è´¥")
        except Error as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            raise
    
    def get_timestamp(self) -> int:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return int(time.time() * 1000)
    
    def safe_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼ˆå¸¦é™é€Ÿå’Œé‡è¯•ï¼‰"""
        # é™é€Ÿæ§åˆ¶
        self.rate_limiter.wait_if_needed()
        
        for attempt in range(self.config.api_retry_count + 1):
            try:
                response = self.session.request(
                    method, url, 
                    timeout=self.config.api_timeout,
                    **kwargs
                )
                response.raise_for_status()
                return response
                
            except requests.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ {url} (å°è¯• {attempt + 1}/{self.config.api_retry_count + 1}): {e}")
                
                if attempt < self.config.api_retry_count:
                    time.sleep(self.config.api_retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                else:
                    self.logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥ {url}: {e}")
                    self.stats['errors'].append({
                        'url': url,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
                    return None
        
        return None
    
    def get_work_detail(self, slug: str) -> Optional[Dict[str, Any]]:
        """è·å–ä½œå“è¯¦æƒ… - group/get/{slug}"""
        url = f"{self.api_base}/api/www/img/group/get/{slug}"
        
        payload = {
            "timestamp": self.get_timestamp()
        }
        
        self.logger.debug(f"è·å–ä½œå“è¯¦æƒ…: {slug}")
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                if data.get('code') == 0:
                    work_data = data.get('data', {})
                    
                    # ä¿å­˜åŸå§‹æ•°æ®
                    if self.config.save_raw_data:
                        raw_file = os.path.join(self.config.raw_data_dir, f'work_detail_{slug}.json')
                        with open(raw_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    return work_data
                else:
                    self.logger.warning(f"ä½œå“è¯¦æƒ…æ¥å£è¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
            except json.JSONDecodeError:
                self.logger.error(f"ä½œå“è¯¦æƒ…å“åº”æ ¼å¼é”™è¯¯: {slug}")
        
        return None
    
    def get_author_detail(self, author_slug: str) -> Optional[Dict[str, Any]]:
        """è·å–ä½œè€…è¯¦æƒ… - author/{slug}"""
        url = f"{self.api_base}/api/www/img/author/{author_slug}"
        
        params = {
            "timestamp": self.get_timestamp()
        }
        
        self.logger.debug(f"è·å–ä½œè€…è¯¦æƒ…: {author_slug}")
        
        response = self.safe_request('POST', url, params=params)
        if response:
            try:
                data = response.json()
                if data.get('code') == 0:
                    author_data = data.get('data', {})
                    
                    # ä¿å­˜åŸå§‹æ•°æ®
                    if self.config.save_raw_data:
                        raw_file = os.path.join(self.config.raw_data_dir, f'author_{author_slug}.json')
                        with open(raw_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    return author_data
                else:
                    self.logger.warning(f"ä½œè€…è¯¦æƒ…æ¥å£è¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
            except json.JSONDecodeError:
                self.logger.error(f"ä½œè€…è¯¦æƒ…å“åº”æ ¼å¼é”™è¯¯: {author_slug}")
        
        return None
    
    def get_work_comments(self, work_id: int, slug: str) -> List[Dict[str, Any]]:
        """è·å–ä½œå“è¯„è®ºï¼ˆå¯é€‰ï¼‰"""
        if not self.config.collect_comments:
            return []
        
        url = f"{self.api_base}/api/www/community/commentList"
        
        payload = {
            "workId": work_id,
            "page": 1,
            "pageSize": 50,
            "sortType": "hot",
            "timestamp": self.get_timestamp()
        }
        
        self.logger.debug(f"è·å–ä½œå“è¯„è®º: {slug}")
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                if data.get('code') == 0:
                    comments = data.get('data', {}).get('list', [])
                    
                    # ä¿å­˜åŸå§‹æ•°æ®
                    if self.config.save_raw_data:
                        raw_file = os.path.join(self.config.raw_data_dir, f'comments_{slug}.json')
                        with open(raw_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    return comments
                else:
                    self.logger.warning(f"è¯„è®ºæ¥å£è¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
            except json.JSONDecodeError:
                self.logger.error(f"è¯„è®ºå“åº”æ ¼å¼é”™è¯¯: {slug}")
        
        return []
    
    def validate_and_default_work_data(self, work_data: Dict[str, Any]) -> Dict[str, Any]:
        """å­—æ®µæ ¡éªŒä¸ç¼ºçœç­–ç•¥ - ä½œå“æ•°æ®"""
        validated = {}
        
        # å¿…å¡«å­—æ®µæ ¡éªŒ
        required_fields = ['slug', 'title']
        for field in required_fields:
            if not work_data.get(field):
                if self.config.strict_validation:
                    self.logger.error(f"ä½œå“ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
                    return {}
                else:
                    self.logger.warning(f"ä½œå“ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        # åŸºç¡€å­—æ®µ
        validated['slug'] = work_data.get('slug', '')
        validated['title'] = work_data.get('title', '')
        validated['published_at'] = self.parse_datetime(work_data.get('publishedAt'))
        
        # æ ‡ç­¾å¤„ç†
        tags = work_data.get('tags', [])
        if isinstance(tags, list):
            validated['tags_json'] = json.dumps(tags, ensure_ascii=False)
        else:
            validated['tags_json'] = json.dumps([], ensure_ascii=False)
        
        # æç¤ºè¯å¤„ç†
        validated['prompt'] = work_data.get('prompt', '') or ''
        validated['negative_prompt'] = work_data.get('negativePrompt', '') or ''
        
        # ç”Ÿæˆå‚æ•°
        validated['sampler'] = work_data.get('sampler', '') or ''
        validated['steps'] = work_data.get('steps', 0) or 0
        validated['cfg_scale'] = float(work_data.get('cfgScale', 0)) or 0.0
        validated['width'] = work_data.get('width', 0) or 0
        validated['height'] = work_data.get('height', 0) or 0
        validated['seed'] = str(work_data.get('seed', '')) or ''
        
        # ç»Ÿè®¡æ•°æ®
        validated['like_count'] = work_data.get('likeCount', 0) or 0
        validated['favorite_count'] = work_data.get('favoriteCount', 0) or 0
        validated['comment_count'] = work_data.get('commentCount', 0) or 0
        
        # æºURL
        validated['source_url'] = work_data.get('sourceUrl', '') or ''
        
        return validated
    
    def validate_and_default_author_data(self, author_data: Dict[str, Any]) -> Dict[str, Any]:
        """å­—æ®µæ ¡éªŒä¸ç¼ºçœç­–ç•¥ - ä½œè€…æ•°æ®"""
        validated = {}
        
        # å¿…å¡«å­—æ®µæ ¡éªŒ
        if not author_data.get('name'):
            if self.config.strict_validation:
                self.logger.error("ä½œè€…ç¼ºå°‘å¿…å¡«å­—æ®µ: name")
                return {}
            else:
                self.logger.warning("ä½œè€…ç¼ºå°‘å¿…å¡«å­—æ®µ: nameï¼Œä½¿ç”¨é»˜è®¤å€¼")
                validated['name'] = 'Unknown'
        
        # åŸºç¡€å­—æ®µ
        validated['external_author_id'] = author_data.get('id', '') or ''
        validated['name'] = author_data.get('name', 'Unknown')
        validated['avatar_url'] = author_data.get('avatar', '') or ''
        validated['profile_url'] = author_data.get('profileUrl', '') or ''
        validated['created_at'] = self.parse_datetime(author_data.get('createdAt'))
        
        return validated
    
    def parse_datetime(self, timestamp: Any) -> Optional[datetime]:
        """è§£ææ—¶é—´æˆ³"""
        if not timestamp:
            return None
        
        try:
            if isinstance(timestamp, (int, float)):
                # æ¯«ç§’æ—¶é—´æˆ³
                if timestamp > 1e10:  # æ¯«ç§’
                    timestamp = timestamp / 1000
                return datetime.fromtimestamp(timestamp, tz=timezone.utc)
            elif isinstance(timestamp, str):
                # ISOæ ¼å¼å­—ç¬¦ä¸²
                return datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        except (ValueError, TypeError):
            self.logger.warning(f"æ— æ³•è§£ææ—¶é—´æˆ³: {timestamp}")
        
        return None
    
    def create_author(self, author_data: Dict[str, Any]) -> Optional[int]:
        """åˆ›å»ºä½œè€…è®°å½•"""
        try:
            cursor = self.connection.cursor()
            
            # æ£€æŸ¥ä½œè€…æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM authors WHERE name = %s"
            cursor.execute(check_sql, (author_data['name'],))
            existing = cursor.fetchone()
            
            if existing:
                self.logger.debug(f"ä½œè€…å·²å­˜åœ¨: {author_data['name']} (ID: {existing[0]})")
                return existing[0]
            
            # æ’å…¥æ–°ä½œè€…
            insert_sql = """
                INSERT INTO authors (external_author_id, name, avatar_url, profile_url, created_at)
                VALUES (%s, %s, %s, %s, %s)
            """
            cursor.execute(insert_sql, (
                author_data['external_author_id'],
                author_data['name'],
                author_data['avatar_url'],
                author_data['profile_url'],
                author_data['created_at']
            ))
            
            author_id = cursor.lastrowid
            self.connection.commit()
            
            self.logger.info(f"åˆ›å»ºä½œè€…æˆåŠŸ: {author_data['name']} (ID: {author_id})")
            self.stats['authors_created'] += 1
            
            return author_id
            
        except Error as e:
            self.logger.error(f"åˆ›å»ºä½œè€…å¤±è´¥: {e}")
            self.connection.rollback()
            return None
    
    def create_work(self, work_data: Dict[str, Any], author_id: Optional[int]) -> Optional[int]:
        """åˆ›å»ºä½œå“è®°å½•"""
        try:
            cursor = self.connection.cursor()
            
            # æ£€æŸ¥ä½œå“æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM works WHERE slug = %s"
            cursor.execute(check_sql, (work_data['slug'],))
            existing = cursor.fetchone()
            
            if existing:
                self.logger.debug(f"ä½œå“å·²å­˜åœ¨: {work_data['slug']} (ID: {existing[0]})")
                return existing[0]
            
            # æ’å…¥æ–°ä½œå“
            insert_sql = """
                INSERT INTO works (
                    slug, title, published_at, tags_json, prompt, negative_prompt,
                    sampler, steps, cfg_scale, width, height, seed,
                    like_count, favorite_count, comment_count, source_url, author_id
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
            """
            cursor.execute(insert_sql, (
                work_data['slug'],
                work_data['title'],
                work_data['published_at'],
                work_data['tags_json'],
                work_data['prompt'],
                work_data['negative_prompt'],
                work_data['sampler'],
                work_data['steps'],
                work_data['cfg_scale'],
                work_data['width'],
                work_data['height'],
                work_data['seed'],
                work_data['like_count'],
                work_data['favorite_count'],
                work_data['comment_count'],
                work_data['source_url'],
                author_id
            ))
            
            work_id = cursor.lastrowid
            self.connection.commit()
            
            self.logger.info(f"åˆ›å»ºä½œå“æˆåŠŸ: {work_data['title']} (ID: {work_id})")
            self.stats['works_created'] += 1
            
            return work_id
            
        except Error as e:
            self.logger.error(f"åˆ›å»ºä½œå“å¤±è´¥: {e}")
            self.connection.rollback()
            return None
    
    def create_comments(self, work_id: int, comments: List[Dict[str, Any]]) -> int:
        """åˆ›å»ºè¯„è®ºè®°å½•"""
        if not comments:
            return 0
        
        created_count = 0
        
        try:
            cursor = self.connection.cursor()
            
            for comment in comments:
                # æ£€æŸ¥è¯„è®ºæ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºå†…å®¹å’Œæ—¶é—´ï¼‰
                comment_content = comment.get('content', '')
                comment_time = self.parse_datetime(comment.get('commentedAt'))
                
                if not comment_content or not comment_time:
                    continue
                
                # ç®€å•çš„é‡å¤æ£€æŸ¥
                check_sql = """
                    SELECT id FROM comments 
                    WHERE work_id = %s AND content = %s AND commented_at = %s
                """
                cursor.execute(check_sql, (work_id, comment_content, comment_time))
                existing = cursor.fetchone()
                
                if existing:
                    continue
                
                # æ’å…¥æ–°è¯„è®º
                insert_sql = """
                    INSERT INTO comments (work_id, commenter_name, commenter_avatar_url, content, commented_at)
                    VALUES (%s, %s, %s, %s, %s)
                """
                cursor.execute(insert_sql, (
                    work_id,
                    comment.get('commenterName', ''),
                    comment.get('commenterAvatar', ''),
                    comment_content,
                    comment_time
                ))
                
                created_count += 1
            
            self.connection.commit()
            
            if created_count > 0:
                self.logger.info(f"åˆ›å»ºè¯„è®ºæˆåŠŸ: {created_count} æ¡")
                self.stats['comments_created'] += created_count
            
        except Error as e:
            self.logger.error(f"åˆ›å»ºè¯„è®ºå¤±è´¥: {e}")
            self.connection.rollback()
        
        return created_count
    
    def process_single_work(self, slug: str) -> bool:
        """å¤„ç†å•ä¸ªä½œå“çš„è¯¦æƒ…é‡‡é›†"""
        try:
            self.logger.info(f"å¼€å§‹å¤„ç†ä½œå“: {slug}")
            
            # 1. è·å–ä½œå“è¯¦æƒ…
            work_detail = self.get_work_detail(slug)
            if not work_detail:
                self.logger.error(f"æ— æ³•è·å–ä½œå“è¯¦æƒ…: {slug}")
                return False
            
            # 2. å­—æ®µæ ¡éªŒä¸ç¼ºçœ
            validated_work = self.validate_and_default_work_data(work_detail)
            if not validated_work:
                if self.config.skip_invalid_works:
                    self.logger.warning(f"è·³è¿‡æ— æ•ˆä½œå“: {slug}")
                    return False
                else:
                    self.logger.error(f"ä½œå“æ•°æ®éªŒè¯å¤±è´¥: {slug}")
                    return False
            
            # 3. è·å–ä½œè€…ä¿¡æ¯
            author_id = None
            if self.config.collect_author_info:
                author_slug = work_detail.get('authorSlug', '')
                if author_slug:
                    author_detail = self.get_author_detail(author_slug)
                    if author_detail:
                        validated_author = self.validate_and_default_author_data(author_detail)
                        if validated_author:
                            author_id = self.create_author(validated_author)
            
            # 4. åˆ›å»ºä½œå“è®°å½•
            work_id = self.create_work(validated_work, author_id)
            if not work_id:
                self.logger.error(f"åˆ›å»ºä½œå“è®°å½•å¤±è´¥: {slug}")
                return False
            
            # 5. è·å–å¹¶åˆ›å»ºè¯„è®ºï¼ˆå¯é€‰ï¼‰
            if self.config.collect_comments and work_detail.get('commentCount', 0) > 0:
                comments = self.get_work_comments(work_id, slug)
                if comments:
                    self.create_comments(work_id, comments)
            
            # 6. å¤„ç†æ¨¡å‹å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.config.collect_model_references:
                self.process_model_references(work_id, work_detail)
            
            self.logger.info(f"ä½œå“å¤„ç†å®Œæˆ: {slug}")
            return True
            
        except Exception as e:
            self.logger.error(f"å¤„ç†ä½œå“å¼‚å¸¸ {slug}: {e}")
            self.stats['errors'].append({
                'slug': slug,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            return False
    
    def process_model_references(self, work_id: int, work_detail: Dict[str, Any]):
        """å¤„ç†æ¨¡å‹å¼•ç”¨"""
        try:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…APIå“åº”ç»“æ„æ¥æå–æ¨¡å‹å¼•ç”¨ä¿¡æ¯
            # ç›®å‰å…ˆé¢„ç•™æ¥å£
            pass
        except Exception as e:
            self.logger.error(f"å¤„ç†æ¨¡å‹å¼•ç”¨å¤±è´¥: {e}")
    
    def collect_details_batch(self, slugs: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡é‡‡é›†è¯¦æƒ…"""
        self.logger.info(f"å¼€å§‹æ‰¹é‡é‡‡é›†è¯¦æƒ…ï¼Œå…± {len(slugs)} ä¸ªä½œå“")
        
        self.stats['total_processed'] = len(slugs)
        self.stats['start_time'] = datetime.now()
        
        # å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            future_to_slug = {
                executor.submit(self.process_single_work, slug): slug 
                for slug in slugs
            }
            
            for future in as_completed(future_to_slug):
                slug = future_to_slug[future]
                try:
                    success = future.result()
                    if success:
                        self.stats['success_count'] += 1
                    else:
                        self.stats['failed_count'] += 1
                except Exception as e:
                    self.logger.error(f"å¤„ç†ä½œå“å¼‚å¸¸ {slug}: {e}")
                    self.stats['failed_count'] += 1
                    self.stats['errors'].append({
                        'slug': slug,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (self.stats['success_count'] / self.stats['total_processed']) * 100 if self.stats['total_processed'] > 0 else 0
        
        self.logger.info(f"æ‰¹é‡é‡‡é›†å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2f}%")
        self.logger.info(f"æˆåŠŸ: {self.stats['success_count']}, å¤±è´¥: {self.stats['failed_count']}")
        
        return self.stats
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é‡‡é›†ç»Ÿè®¡"""
        return {
            'total_processed': self.stats['total_processed'],
            'success_count': self.stats['success_count'],
            'failed_count': self.stats['failed_count'],
            'success_rate': (self.stats['success_count'] / self.stats['total_processed']) * 100 if self.stats['total_processed'] > 0 else 0,
            'authors_created': self.stats['authors_created'],
            'works_created': self.stats['works_created'],
            'comments_created': self.stats['comments_created'],
            'start_time': self.stats['start_time'].isoformat(),
            'duration': (datetime.now() - self.stats['start_time']).total_seconds(),
            'error_count': len(self.stats['errors']),
            'errors': self.stats['errors']
        }
    
    def save_stats_to_file(self, filename: str = 'collection_stats.json'):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        stats = self.get_collection_stats()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {filename}")
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection and self.connection.is_connected():
            self.connection.close()
            self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¢å¼ºç‰ˆè¯¦æƒ…é‡‡é›†å™¨"""
    # æµ‹è¯•ç”¨çš„slugåˆ—è¡¨
    test_slugs = [
        "test-slug-1",
        "test-slug-2"
    ]
    
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        
        # éªŒè¯é…ç½®
        errors = config.validate()
        if errors:
            print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                print(f"   - {error}")
            return
        
        # åˆ›å»ºé‡‡é›†å™¨
        collector = EnhancedDetailCollector(config)
        
        # æ‰¹é‡é‡‡é›†è¯¦æƒ…
        stats = collector.collect_details_batch(test_slugs)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("\nâœ… è¯¦æƒ…é‡‡é›†å®Œæˆï¼")
        print(f"ğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†æ•°: {stats['total_processed']}")
        print(f"   æˆåŠŸæ•°: {stats['success_count']}")
        print(f"   å¤±è´¥æ•°: {stats['failed_count']}")
        print(f"   æˆåŠŸç‡: {stats.get('success_rate', 0):.2f}%")
        print(f"   åˆ›å»ºä½œè€…: {stats['authors_created']}")
        print(f"   åˆ›å»ºä½œå“: {stats['works_created']}")
        print(f"   åˆ›å»ºè¯„è®º: {stats['comments_created']}")
        print(f"   é”™è¯¯æ•°: {len(stats['errors'])}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        collector.save_stats_to_file()
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        print(f"\nâŒ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if 'collector' in locals():
            collector.close()

if __name__ == "__main__":
    main()
