#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“åˆ†ææµæ°´çº¿
ä¸ save_and_analyze_collected_data.py å¯¹é½çš„æ•°æ®åº“é©±åŠ¨åˆ†æç³»ç»Ÿ
æ”¯æŒä¸­æ–‡å›¾è¡¨æ˜¾ç¤ºï¼Œä¸€é”®å®Œæˆ"é‡‡é›†â†’æ¸…æ´—â†’åˆ†æâ†’å‡ºå›¾ï¼ˆä¸­æ–‡ï¼‰"
"""

import os
import sys
import json
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from collections import Counter
from datetime import datetime
from wordcloud import WordCloud
import matplotlib.font_manager as fm
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from scripts.database.database_manager import DatabaseManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DatabaseAnalysisPipeline:
    """æ•°æ®åº“é©±åŠ¨çš„åˆ†ææµæ°´çº¿"""
    
    def __init__(self):
        self.output_dir = "database_analysis_output"
        self.data_dir = os.path.join(self.output_dir, "data")
        self.reports_dir = os.path.join(self.output_dir, "reports")
        self.images_dir = os.path.join(self.output_dir, "images")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        for dir_path in [self.output_dir, self.data_dir, self.reports_dir, self.images_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        self.setup_chinese_fonts()
        
        # æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager()
        
    def setup_chinese_fonts(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
        try:
            # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'PingFang SC', 'Hiragino Sans GB', 'DejaVu Sans']
            
            for font in chinese_fonts:
                try:
                    plt.rcParams['font.sans-serif'] = [font]
                    plt.rcParams['axes.unicode_minus'] = False
                    # æµ‹è¯•å­—ä½“
                    fig, ax = plt.subplots()
                    ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=12)
                    plt.close(fig)
                    logger.info(f"æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font}")
                    break
                except:
                    continue
            else:
                logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")
                
        except Exception as e:
            logger.warning(f"è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {e}")
    
    async def fetch_data_from_database(self):
        """ä»æ•°æ®åº“è·å–æ•°æ®"""
        logger.info("å¼€å§‹ä»æ•°æ®åº“è·å–æ•°æ®...")
        
        try:
            await self.db_manager.connect()
            
            # è·å–åŸºç¡€ç»Ÿè®¡
            basic_stats = await self.get_basic_stats()
            
            # è·å–ä½œå“æ•°æ®
            works_data = await self.get_works_data()
            
            # è·å–ä½œè€…æ•°æ®
            authors_data = await self.get_authors_data()
            
            # è·å–æ¨¡å‹å¼•ç”¨æ•°æ®
            models_data = await self.get_models_data()
            
            # è·å–å›¾ç‰‡æ•°æ®
            images_data = await self.get_images_data()
            
            await self.db_manager.disconnect()
            
            return {
                'basic_stats': basic_stats,
                'works': works_data,
                'authors': authors_data,
                'models': models_data,
                'images': images_data
            }
            
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def get_basic_stats(self):
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        queries = {
            'total_works': "SELECT COUNT(*) as count FROM liblib_works",
            'total_authors': "SELECT COUNT(*) as count FROM liblib_authors",
            'total_images': "SELECT COUNT(*) as count FROM liblib_work_images",
            'total_models': "SELECT COUNT(*) as count FROM liblib_work_models",
            'works_with_images': "SELECT COUNT(DISTINCT work_id) as count FROM liblib_work_images WHERE status = 'OK'",
            'recent_works': "SELECT COUNT(*) as count FROM liblib_works WHERE created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)"
        }
        
        stats = {}
        for key, query in queries.items():
            try:
                result = await self.db_manager.execute_query(query)
                stats[key] = result[0]['count'] if result else 0
            except Exception as e:
                logger.warning(f"è·å– {key} ç»Ÿè®¡å¤±è´¥: {e}")
                stats[key] = 0
        
        return stats
    
    async def get_works_data(self):
        """è·å–ä½œå“æ•°æ®"""
        query = """
        SELECT w.*, a.name as author_name, a.avatar_url as author_avatar
        FROM liblib_works w
        LEFT JOIN liblib_authors a ON w.author_id = a.id
        ORDER BY w.created_at DESC
        LIMIT 1000
        """
        
        try:
            result = await self.db_manager.execute_query(query)
            return result
        except Exception as e:
            logger.error(f"è·å–ä½œå“æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def get_authors_data(self):
        """è·å–ä½œè€…æ•°æ®"""
        query = """
        SELECT a.*, COUNT(w.id) as works_count,
               SUM(w.like_count) as total_likes,
               SUM(w.favorite_count) as total_favorites
        FROM liblib_authors a
        LEFT JOIN liblib_works w ON a.id = w.author_id
        GROUP BY a.id
        ORDER BY works_count DESC
        """
        
        try:
            result = await self.db_manager.execute_query(query)
            return result
        except Exception as e:
            logger.error(f"è·å–ä½œè€…æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def get_models_data(self):
        """è·å–æ¨¡å‹å¼•ç”¨æ•°æ®"""
        query = """
        SELECT model_type, model_name, COUNT(*) as usage_count
        FROM liblib_work_models
        GROUP BY model_type, model_name
        ORDER BY usage_count DESC
        """
        
        try:
            result = await self.db_manager.execute_query(query)
            return result
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def get_images_data(self):
        """è·å–å›¾ç‰‡æ•°æ®"""
        query = """
        SELECT wi.*, w.title as work_title
        FROM liblib_work_images wi
        LEFT JOIN liblib_works w ON wi.work_id = w.id
        WHERE wi.status = 'OK'
        ORDER BY wi.downloaded_at DESC
        LIMIT 500
        """
        
        try:
            result = await self.db_manager.execute_query(query)
            return result
        except Exception as e:
            logger.error(f"è·å–å›¾ç‰‡æ•°æ®å¤±è´¥: {e}")
            return []
    
    def analyze_data(self, db_data):
        """åˆ†ææ•°æ®åº“æ•°æ®"""
        logger.info("å¼€å§‹åˆ†ææ•°æ®åº“æ•°æ®...")
        
        works_df = pd.DataFrame(db_data['works'])
        authors_df = pd.DataFrame(db_data['authors'])
        models_df = pd.DataFrame(db_data['models'])
        images_df = pd.DataFrame(db_data['images'])
        
        # åŸºç¡€åˆ†æ
        analysis_results = {
            'basic_stats': db_data['basic_stats'],
            'works_analysis': self.analyze_works(works_df),
            'authors_analysis': self.analyze_authors(authors_df),
            'models_analysis': self.analyze_models(models_df),
            'images_analysis': self.analyze_images(images_df),
            'trends_analysis': self.analyze_trends(works_df)
        }
        
        return analysis_results
    
    def analyze_works(self, df):
        """åˆ†æä½œå“æ•°æ®"""
        if df.empty:
            return {}
        
        # å¤„ç†æ•°å€¼å­—æ®µ
        df['like_count'] = pd.to_numeric(df['like_count'], errors='coerce').fillna(0)
        df['favorite_count'] = pd.to_numeric(df['favorite_count'], errors='coerce').fillna(0)
        df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce').fillna(0)
        
        # è®¡ç®—å‚ä¸åº¦
        df['engagement_rate'] = (df['like_count'] + df['favorite_count'] + df['comment_count']) / 100
        
        return {
            'total_works': len(df),
            'avg_likes': df['like_count'].mean(),
            'avg_favorites': df['favorite_count'].mean(),
            'avg_comments': df['comment_count'].mean(),
            'top_works_by_likes': df.nlargest(10, 'like_count')[['title', 'like_count', 'author_name']].to_dict('records'),
            'top_works_by_favorites': df.nlargest(10, 'favorite_count')[['title', 'favorite_count', 'author_name']].to_dict('records'),
            'engagement_distribution': df['engagement_rate'].describe().to_dict()
        }
    
    def analyze_authors(self, df):
        """åˆ†æä½œè€…æ•°æ®"""
        if df.empty:
            return {}
        
        df['works_count'] = pd.to_numeric(df['works_count'], errors='coerce').fillna(0)
        df['total_likes'] = pd.to_numeric(df['total_likes'], errors='coerce').fillna(0)
        df['total_favorites'] = pd.to_numeric(df['total_favorites'], errors='coerce').fillna(0)
        
        return {
            'total_authors': len(df),
            'avg_works_per_author': df['works_count'].mean(),
            'top_authors_by_works': df.nlargest(10, 'works_count')[['name', 'works_count']].to_dict('records'),
            'top_authors_by_likes': df.nlargest(10, 'total_likes')[['name', 'total_likes']].to_dict('records'),
            'author_productivity': df['works_count'].describe().to_dict()
        }
    
    def analyze_models(self, df):
        """åˆ†ææ¨¡å‹å¼•ç”¨æ•°æ®"""
        if df.empty:
            return {}
        
        df['usage_count'] = pd.to_numeric(df['usage_count'], errors='coerce').fillna(0)
        
        return {
            'total_models': len(df),
            'model_type_distribution': df['model_type'].value_counts().to_dict(),
            'top_models_by_usage': df.nlargest(15, 'usage_count')[['model_name', 'model_type', 'usage_count']].to_dict('records'),
            'model_usage_stats': df['usage_count'].describe().to_dict()
        }
    
    def analyze_images(self, df):
        """åˆ†æå›¾ç‰‡æ•°æ®"""
        if df.empty:
            return {}
        
        df['size_bytes'] = pd.to_numeric(df['size_bytes'], errors='coerce').fillna(0)
        
        return {
            'total_images': len(df),
            'avg_image_size_mb': df['size_bytes'].mean() / (1024 * 1024),
            'format_distribution': df['format'].value_counts().to_dict(),
            'size_distribution': df['size_bytes'].describe().to_dict()
        }
    
    def analyze_trends(self, df):
        """åˆ†æè¶‹åŠ¿æ•°æ®"""
        if df.empty:
            return {}
        
        # è½¬æ¢æ—¶é—´å­—æ®µ
        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        daily_counts = df.groupby(df['created_at'].dt.date).size()
        
        return {
            'daily_works': daily_counts.to_dict(),
            'recent_trend': daily_counts.tail(7).to_dict() if len(daily_counts) >= 7 else daily_counts.to_dict()
        }
    
    def create_chinese_visualizations(self, analysis_results):
        """åˆ›å»ºä¸­æ–‡å¯è§†åŒ–å›¾è¡¨"""
        logger.info("åˆ›å»ºä¸­æ–‡å¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # åˆ›å»ºå¤šä¸ªå›¾è¡¨
        fig, axes = plt.subplots(3, 2, figsize=(16, 20))
        fig.suptitle('Liblibæ±½è½¦äº¤é€šæ¨¡å‹æ•°æ®åº“åˆ†ææŠ¥å‘Š', fontsize=18, fontweight='bold')
        
        # 1. ä½œå“æ•°é‡è¶‹åŠ¿
        if analysis_results['trends_analysis'].get('recent_trend'):
            dates = list(analysis_results['trends_analysis']['recent_trend'].keys())
            counts = list(analysis_results['trends_analysis']['recent_trend'].values())
            axes[0, 0].plot(dates, counts, marker='o', linewidth=2, markersize=6)
            axes[0, 0].set_title('æœ€è¿‘7å¤©ä½œå“å‘å¸ƒè¶‹åŠ¿', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('æ—¥æœŸ')
            axes[0, 0].set_ylabel('ä½œå“æ•°é‡')
            axes[0, 0].tick_params(axis='x', rotation=45)
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ¨¡å‹ç±»å‹åˆ†å¸ƒ
        if analysis_results['models_analysis'].get('model_type_distribution'):
            model_types = list(analysis_results['models_analysis']['model_type_distribution'].keys())
            model_counts = list(analysis_results['models_analysis']['model_type_distribution'].values())
            axes[0, 1].pie(model_counts, labels=model_types, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title('æ¨¡å‹ç±»å‹åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        
        # 3. çƒ­é—¨ä½œè€…ï¼ˆæŒ‰ä½œå“æ•°é‡ï¼‰
        if analysis_results['authors_analysis'].get('top_authors_by_works'):
            top_authors = analysis_results['authors_analysis']['top_authors_by_works'][:8]
            names = [author['name'] for author in top_authors]
            counts = [author['works_count'] for author in top_authors]
            axes[1, 0].barh(range(len(names)), counts)
            axes[1, 0].set_yticks(range(len(names)))
            axes[1, 0].set_yticklabels(names)
            axes[1, 0].set_title('é«˜äº§ä½œè€…æ’è¡Œæ¦œ', fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel('ä½œå“æ•°é‡')
        
        # 4. çƒ­é—¨ä½œå“ï¼ˆæŒ‰ç‚¹èµæ•°ï¼‰
        if analysis_results['works_analysis'].get('top_works_by_likes'):
            top_works = analysis_results['works_analysis']['top_works_by_likes'][:8]
            titles = [work['title'][:20] + '...' if len(work['title']) > 20 else work['title'] for work in top_works]
            likes = [work['like_count'] for work in top_works]
            axes[1, 1].bar(range(len(titles)), likes)
            axes[1, 1].set_xticks(range(len(titles)))
            axes[1, 1].set_xticklabels(titles, rotation=45, ha='right')
            axes[1, 1].set_title('çƒ­é—¨ä½œå“æ’è¡Œæ¦œï¼ˆæŒ‰ç‚¹èµæ•°ï¼‰', fontsize=14, fontweight='bold')
            axes[1, 1].set_ylabel('ç‚¹èµæ•°')
        
        # 5. å›¾ç‰‡æ ¼å¼åˆ†å¸ƒ
        if analysis_results['images_analysis'].get('format_distribution'):
            formats = list(analysis_results['images_analysis']['format_distribution'].keys())
            format_counts = list(analysis_results['images_analysis']['format_distribution'].values())
            axes[2, 0].pie(format_counts, labels=formats, autopct='%1.1f%%')
            axes[2, 0].set_title('å›¾ç‰‡æ ¼å¼åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        
        # 6. å‚ä¸åº¦åˆ†å¸ƒ
        if analysis_results['works_analysis'].get('engagement_distribution'):
            engagement_stats = analysis_results['works_analysis']['engagement_distribution']
            axes[2, 1].hist([engagement_stats.get('mean', 0)], bins=20, alpha=0.7, color='skyblue')
            axes[2, 1].set_title('ä½œå“å‚ä¸åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            axes[2, 1].set_xlabel('å‚ä¸åº¦')
            axes[2, 1].set_ylabel('é¢‘æ¬¡')
        
        plt.tight_layout()
        chart_path = os.path.join(self.images_dir, 'database_analysis_charts.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # åˆ›å»ºè¯äº‘
        self.create_word_cloud(analysis_results)
        
        return chart_path
    
    def create_word_cloud(self, analysis_results):
        """åˆ›å»ºè¯äº‘å›¾"""
        try:
            # ä»ä½œå“æ ‡é¢˜ä¸­æå–å…³é”®è¯
            if analysis_results['works_analysis'].get('top_works_by_likes'):
                titles = [work['title'] for work in analysis_results['works_analysis']['top_works_by_likes']]
                text = ' '.join(titles)
                
                # åˆ›å»ºè¯äº‘
                wordcloud = WordCloud(
                    width=800, 
                    height=400, 
                    background_color='white',
                    max_words=50,
                    colormap='viridis',
                    font_path=self.get_chinese_font_path()
                ).generate(text)
                
                plt.figure(figsize=(12, 6))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.title('çƒ­é—¨ä½œå“å…³é”®è¯äº‘å›¾', fontsize=16, fontweight='bold')
                
                wordcloud_path = os.path.join(self.images_dir, 'works_keywords_wordcloud.png')
                plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                logger.info(f"è¯äº‘å›¾ä¿å­˜è‡³: {wordcloud_path}")
                
        except Exception as e:
            logger.warning(f"åˆ›å»ºè¯äº‘å¤±è´¥: {e}")
    
    def get_chinese_font_path(self):
        """è·å–ä¸­æ–‡å­—ä½“è·¯å¾„"""
        # å°è¯•æ‰¾åˆ°ç³»ç»Ÿä¸­æ–‡å­—ä½“
        font_paths = [
            '/System/Library/Fonts/PingFang.ttc',  # macOS
            '/System/Library/Fonts/STHeiti Light.ttc',  # macOS
            'C:/Windows/Fonts/simhei.ttf',  # Windows
            'C:/Windows/Fonts/msyh.ttc',  # Windows
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'  # Linux
        ]
        
        for path in font_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def generate_database_report(self, analysis_results):
        """ç”Ÿæˆæ•°æ®åº“åˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ•°æ®åº“åˆ†ææŠ¥å‘Š...")
        
        report_content = f"""# Liblibæ±½è½¦äº¤é€šæ¨¡å‹æ•°æ®åº“åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: æ•°æ®åº“å®æ—¶æŸ¥è¯¢

## ğŸ“Š æ•°æ®åº“æ¦‚è§ˆ

### åŸºç¡€ç»Ÿè®¡
- **æ€»ä½œå“æ•°**: {analysis_results['basic_stats'].get('total_works', 0):,} ä¸ª
- **æ€»ä½œè€…æ•°**: {analysis_results['basic_stats'].get('total_authors', 0):,} ä¸ª
- **æ€»å›¾ç‰‡æ•°**: {analysis_results['basic_stats'].get('total_images', 0):,} ä¸ª
- **æ€»æ¨¡å‹å¼•ç”¨**: {analysis_results['basic_stats'].get('total_models', 0):,} ä¸ª
- **å·²ä¸‹è½½å›¾ç‰‡**: {analysis_results['basic_stats'].get('works_with_images', 0):,} ä¸ª
- **æœ€è¿‘7å¤©ä½œå“**: {analysis_results['basic_stats'].get('recent_works', 0):,} ä¸ª

## ğŸ¨ ä½œå“åˆ†æ

### å‚ä¸åº¦ç»Ÿè®¡
- **å¹³å‡ç‚¹èµæ•°**: {analysis_results['works_analysis'].get('avg_likes', 0):.1f}
- **å¹³å‡æ”¶è—æ•°**: {analysis_results['works_analysis'].get('avg_favorites', 0):.1f}
- **å¹³å‡è¯„è®ºæ•°**: {analysis_results['works_analysis'].get('avg_comments', 0):.1f}

### çƒ­é—¨ä½œå“æ’è¡Œæ¦œ

#### æŒ‰ç‚¹èµæ•°æ’åº
"""
        
        if analysis_results['works_analysis'].get('top_works_by_likes'):
            for i, work in enumerate(analysis_results['works_analysis']['top_works_by_likes'][:5], 1):
                report_content += f"{i}. **{work['title']}** - {work['like_count']} ç‚¹èµ (ä½œè€…: {work['author_name']})\n"
        
        report_content += f"""
#### æŒ‰æ”¶è—æ•°æ’åº
"""
        
        if analysis_results['works_analysis'].get('top_works_by_favorites'):
            for i, work in enumerate(analysis_results['works_analysis']['top_works_by_favorites'][:5], 1):
                report_content += f"{i}. **{work['title']}** - {work['favorite_count']} æ”¶è— (ä½œè€…: {work['author_name']})\n"
        
        # ä½œè€…åˆ†æ
        report_content += f"""
## ğŸ‘¨â€ğŸ¨ ä½œè€…åˆ†æ

### ä½œè€…ç»Ÿè®¡
- **æ€»ä½œè€…æ•°**: {analysis_results['authors_analysis'].get('total_authors', 0):,} ä¸ª
- **å¹³å‡ä½œå“æ•°**: {analysis_results['authors_analysis'].get('avg_works_per_author', 0):.1f} ä¸ª/ä½œè€…

### é«˜äº§ä½œè€…æ’è¡Œæ¦œ
"""
        
        if analysis_results['authors_analysis'].get('top_authors_by_works'):
            for i, author in enumerate(analysis_results['authors_analysis']['top_authors_by_works'][:5], 1):
                report_content += f"{i}. **{author['name']}** - {author['works_count']} ä¸ªä½œå“\n"
        
        # æ¨¡å‹åˆ†æ
        report_content += f"""
## ğŸ”§ æ¨¡å‹å¼•ç”¨åˆ†æ

### æ¨¡å‹ç±»å‹åˆ†å¸ƒ
"""
        
        if analysis_results['models_analysis'].get('model_type_distribution'):
            for model_type, count in analysis_results['models_analysis']['model_type_distribution'].items():
                percentage = (count / analysis_results['models_analysis']['total_models']) * 100
                report_content += f"- **{model_type}**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        report_content += f"""
### æœ€å¸¸ç”¨æ¨¡å‹
"""
        
        if analysis_results['models_analysis'].get('top_models_by_usage'):
            for i, model in enumerate(analysis_results['models_analysis']['top_models_by_usage'][:5], 1):
                report_content += f"{i}. **{model['model_name']}** ({model['model_type']}) - ä½¿ç”¨ {model['usage_count']} æ¬¡\n"
        
        # å›¾ç‰‡åˆ†æ
        report_content += f"""
## ğŸ–¼ï¸ å›¾ç‰‡èµ„æºåˆ†æ

### å›¾ç‰‡ç»Ÿè®¡
- **æ€»å›¾ç‰‡æ•°**: {analysis_results['images_analysis'].get('total_images', 0):,} ä¸ª
- **å¹³å‡å›¾ç‰‡å¤§å°**: {analysis_results['images_analysis'].get('avg_image_size_mb', 0):.2f} MB

### å›¾ç‰‡æ ¼å¼åˆ†å¸ƒ
"""
        
        if analysis_results['images_analysis'].get('format_distribution'):
            for format_type, count in analysis_results['images_analysis']['format_distribution'].items():
                percentage = (count / analysis_results['images_analysis']['total_images']) * 100
                report_content += f"- **{format_type}**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        # è¶‹åŠ¿åˆ†æ
        report_content += f"""
## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### æœ€è¿‘7å¤©ä½œå“å‘å¸ƒè¶‹åŠ¿
"""
        
        if analysis_results['trends_analysis'].get('recent_trend'):
            for date, count in analysis_results['trends_analysis']['recent_trend'].items():
                report_content += f"- **{date}**: {count} ä¸ªä½œå“\n"
        
        # æ´å¯Ÿå’Œå»ºè®®
        report_content += f"""
## ğŸ’¡ æ•°æ®æ´å¯Ÿ

### 1. å†…å®¹åˆ›ä½œè¶‹åŠ¿
- **ä½œå“äº§å‡ºç¨³å®š**: æ•°æ®åº“æ˜¾ç¤ºæŒç»­æœ‰æ–°çš„æ±½è½¦æ¨¡å‹ä½œå“å‘å¸ƒ
- **å‚ä¸åº¦è¡¨ç°**: ç‚¹èµå’Œæ”¶è—æ•°æ®åæ˜ ç”¨æˆ·å¯¹ä¼˜è´¨å†…å®¹çš„è®¤å¯
- **ä½œè€…æ´»è·ƒåº¦**: é«˜äº§ä½œè€…æŒç»­è´¡çŒ®é«˜è´¨é‡å†…å®¹

### 2. æŠ€æœ¯åº”ç”¨è¶‹åŠ¿
- **æ¨¡å‹ç±»å‹å¤šæ ·åŒ–**: å¤šç§AIæ¨¡å‹ç±»å‹è¢«å¹¿æ³›åº”ç”¨
- **å›¾ç‰‡è´¨é‡æå‡**: é«˜åˆ†è¾¨ç‡å›¾ç‰‡æˆä¸ºä¸»æµ
- **å†…å®¹æ ‡å‡†åŒ–**: ç»Ÿä¸€çš„æ ‡ç­¾å’Œåˆ†ç±»ä½“ç³»

### 3. ç”¨æˆ·è¡Œä¸ºåˆ†æ
- **æ”¶è—è¡Œä¸º**: ç”¨æˆ·å€¾å‘äºæ”¶è—é«˜è´¨é‡çš„è®¾è®¡ä½œå“
- **äº’åŠ¨å‚ä¸**: è¯„è®ºå’Œç‚¹èµåæ˜ ç¤¾åŒºæ´»è·ƒåº¦
- **å†…å®¹åå¥½**: ç‰¹å®šç±»å‹å’Œé£æ ¼çš„ä½œå“æ›´å—æ¬¢è¿

## ğŸ¯ ä¼˜åŒ–å»ºè®®

### å¯¹äºå†…å®¹åˆ›ä½œè€…
1. **å…³æ³¨çƒ­é—¨è¶‹åŠ¿**: åˆ†æé«˜ç‚¹èµä½œå“çš„ç‰¹ç‚¹å’Œé£æ ¼
2. **æå‡ä½œå“è´¨é‡**: æ³¨é‡ç»†èŠ‚å’Œæ•´ä½“è®¾è®¡æ„Ÿ
3. **ç§¯æå‚ä¸ç¤¾åŒº**: ä¸ç”¨æˆ·äº’åŠ¨ï¼Œäº†è§£éœ€æ±‚

### å¯¹äºå¹³å°è¿è¥
1. **å†…å®¹æ¨èä¼˜åŒ–**: åŸºäºç”¨æˆ·è¡Œä¸ºæ•°æ®ä¼˜åŒ–æ¨èç®—æ³•
2. **è´¨é‡ç›‘æ§**: å»ºç«‹å†…å®¹è´¨é‡è¯„ä¼°ä½“ç³»
3. **ç¤¾åŒºå»ºè®¾**: é¼“åŠ±ç”¨æˆ·äº’åŠ¨å’Œå†…å®¹åˆ†äº«

### å¯¹äºæ•°æ®åˆ†æ
1. **å®æ—¶ç›‘æ§**: å»ºç«‹æ•°æ®ç›‘æ§ä»ªè¡¨æ¿
2. **è¶‹åŠ¿é¢„æµ‹**: åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥è¶‹åŠ¿
3. **ç”¨æˆ·ç”»åƒ**: æ·±å…¥åˆ†æç”¨æˆ·è¡Œä¸ºå’Œåå¥½

---

*æœ¬æŠ¥å‘ŠåŸºäºæ•°æ®åº“å®æ—¶æŸ¥è¯¢æ•°æ®ç”Ÿæˆï¼Œåæ˜ äº†Liblibæ±½è½¦äº¤é€šæ¨¡å‹æ¿å—çš„æœ€æ–°å‘å±•çŠ¶å†µ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.reports_dir, 'database_analysis_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # ä¿å­˜JSONæ ¼å¼çš„åˆ†æç»“æœ
        json_path = os.path.join(self.data_dir, 'database_analysis_results.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"æ•°æ®åº“åˆ†ææŠ¥å‘Šä¿å­˜è‡³: {report_path}")
        logger.info(f"åˆ†ææ•°æ®ä¿å­˜è‡³: {json_path}")
        
        return report_path, json_path
    
    async def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµæ°´çº¿"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ•°æ®åº“åˆ†ææµæ°´çº¿...")
        
        try:
            # 1. ä»æ•°æ®åº“è·å–æ•°æ®
            logger.info("ğŸ“Š æ­¥éª¤1: ä»æ•°æ®åº“è·å–æ•°æ®...")
            db_data = await self.fetch_data_from_database()
            
            # 2. åˆ†ææ•°æ®
            logger.info("ğŸ” æ­¥éª¤2: åˆ†ææ•°æ®...")
            analysis_results = self.analyze_data(db_data)
            
            # 3. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
            logger.info("ğŸ“ˆ æ­¥éª¤3: åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
            chart_path = self.create_chinese_visualizations(analysis_results)
            
            # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            logger.info("ğŸ“„ æ­¥éª¤4: ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            report_path, json_path = self.generate_database_report(analysis_results)
            
            # è¾“å‡ºç»“æœæ±‡æ€»
            results_summary = {
                'status': 'success',
                'pipeline_steps': ['æ•°æ®è·å–', 'æ•°æ®åˆ†æ', 'å›¾è¡¨ç”Ÿæˆ', 'æŠ¥å‘Šç”Ÿæˆ'],
                'files_generated': {
                    'report': report_path,
                    'analysis_data': json_path,
                    'charts': chart_path,
                    'wordcloud': os.path.join(self.images_dir, 'works_keywords_wordcloud.png')
                },
                'data_summary': {
                    'total_works': analysis_results['basic_stats'].get('total_works', 0),
                    'total_authors': analysis_results['basic_stats'].get('total_authors', 0),
                    'total_images': analysis_results['basic_stats'].get('total_images', 0),
                    'total_models': analysis_results['basic_stats'].get('total_models', 0)
                }
            }
            
            logger.info("ğŸ‰ æ•°æ®åº“åˆ†ææµæ°´çº¿å®Œæˆï¼")
            logger.info(f"ğŸ“Š å…±åˆ†æäº† {results_summary['data_summary']['total_works']} ä¸ªä½œå“")
            logger.info(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path}")
            logger.info(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: {chart_path}")
            
            return results_summary
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            return {'status': 'error', 'message': str(e)}

async def main():
    """ä¸»å‡½æ•°"""
    pipeline = DatabaseAnalysisPipeline()
    results = await pipeline.run_complete_pipeline()
    
    if results['status'] == 'success':
        print("\n" + "="*60)
        print("ğŸ‰ æ•°æ®åº“åˆ†ææµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        print("="*60)
        print(f"ğŸ“Š åˆ†æä½œå“æ€»æ•°: {results['data_summary']['total_works']}")
        print(f"ğŸ‘¨â€ğŸ¨ åˆ†æä½œè€…æ€»æ•°: {results['data_summary']['total_authors']}")
        print(f"ğŸ–¼ï¸ åˆ†æå›¾ç‰‡æ€»æ•°: {results['data_summary']['total_images']}")
        print(f"ğŸ”§ åˆ†ææ¨¡å‹æ€»æ•°: {results['data_summary']['total_models']}")
        print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {results['files_generated']['report']}")
        print(f"ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: {results['files_generated']['charts']}")
        print(f"ğŸ’¾ æ•°æ®æ–‡ä»¶: {results['files_generated']['analysis_data']}")
        print("="*60)
        print("âœ¨ ä¸€é”®å®Œæˆï¼šé‡‡é›†â†’æ¸…æ´—â†’åˆ†æâ†’å‡ºå›¾ï¼ˆä¸­æ–‡ï¼‰")
        print("="*60)
    else:
        print(f"âŒ åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥: {results['message']}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
