#!/usr/bin/env python3
"""
LiblibAI æ±½è½¦äº¤é€šæ¨¡å‹å®Œæ•´é‡‡é›†å™¨
ä¸ºè®¾è®¡å¸ˆæä¾›è¶‹åŠ¿æ´å¯Ÿçš„ä¸“ä¸šé‡‡é›†å·¥å…·
"""

import requests
import json
import time
import os
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, urljoin
import re
from datetime import datetime, timezone
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('car_models_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LiblibCarModelsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.liblib.art/',
            'Origin': 'https://www.liblib.art'
        })
        
        self.base_url = 'https://www.liblib.art'
        self.api_base = 'https://api2.liblib.art'
        self.image_base = 'https://liblibai-online.liblib.cloud'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'car_models_complete'
        self.images_dir = os.path.join(self.output_dir, 'images')
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.images_dir, exist_ok=True)
        
        self.models_data = []
        self.collected_models = set()
        
    def get_timestamp(self) -> int:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return int(time.time() * 1000)
    
    def safe_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè¯·æ±‚"""
        try:
            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            time.sleep(2)
            return None
    
    def get_car_models_list(self, page: int = 1, page_size: int = 24) -> Dict[str, Any]:
        """è·å–æ±½è½¦äº¤é€šåˆ†ç±»æ¨¡å‹åˆ—è¡¨"""
        url = f"{self.api_base}/api/www/model/list"
        
        payload = {
            "categories": ["æ±½è½¦äº¤é€š"],  # æ±½è½¦äº¤é€šåˆ†ç±»
            "page": page,
            "pageSize": page_size,
            "sortType": "recommend",  # æ¨èæ’åº
            "modelType": "",
            "nsfw": False
        }
        
        logger.info(f"è·å–ç¬¬ {page} é¡µæ±½è½¦äº¤é€šæ¨¡å‹åˆ—è¡¨...")
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                return response.json()
            except json.JSONDecodeError:
                logger.error("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
        
        return {}
    
    def get_model_detail(self, model_uuid: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.api_base}/api/www/model/getByUuid/{model_uuid}"
        params = {"timestamp": self.get_timestamp()}
        
        response = self.safe_request('POST', url, params=params)
        if response:
            try:
                return response.json()
            except json.JSONDecodeError:
                logger.error(f"æ¨¡å‹ {model_uuid} è¯¦æƒ…å“åº”æ ¼å¼é”™è¯¯")
        
        return None
    
    def get_model_versions(self, model_uuid: str) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯"""
        url = f"{self.api_base}/api/www/model-version/modelVersion/listByIds"
        payload = {
            "modelIds": [model_uuid],
            "timestamp": self.get_timestamp()
        }
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                return data.get('data', [])
            except json.JSONDecodeError:
                logger.error(f"æ¨¡å‹ç‰ˆæœ¬ {model_uuid} å“åº”æ ¼å¼é”™è¯¯")
        
        return []
    
    def get_model_author(self, model_uuid: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹ä½œè€…ä¿¡æ¯"""
        url = f"{self.api_base}/api/www/model/author/{model_uuid}"
        params = {"timestamp": self.get_timestamp()}
        
        response = self.safe_request('POST', url, params=params)
        if response:
            try:
                return response.json()
            except json.JSONDecodeError:
                logger.error(f"ä½œè€…ä¿¡æ¯ {model_uuid} å“åº”æ ¼å¼é”™è¯¯")
        
        return None
    
    def get_model_comments(self, model_uuid: str) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹è¯„è®º"""
        url = f"{self.api_base}/api/www/community/commentList"
        payload = {
            "modelId": model_uuid,
            "page": 1,
            "pageSize": 50,
            "sortType": "hot",
            "timestamp": self.get_timestamp()
        }
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                return data.get('data', {}).get('list', [])
            except json.JSONDecodeError:
                logger.error(f"è¯„è®º {model_uuid} å“åº”æ ¼å¼é”™è¯¯")
        
        return []
    
    def get_model_returns(self, model_uuid: str) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹è¿”å›¾ï¼ˆç”¨æˆ·ä½œå“ï¼‰"""
        url = f"{self.api_base}/api/www/community/returnPicList"
        payload = {
            "modelId": model_uuid,
            "page": 1,
            "pageSize": 20,
            "sortType": "hot",
            "timestamp": self.get_timestamp()
        }
        
        response = self.safe_request('POST', url, json=payload)
        if response:
            try:
                data = response.json()
                return data.get('data', {}).get('list', [])
            except json.JSONDecodeError:
                logger.error(f"è¿”å›¾ {model_uuid} å“åº”æ ¼å¼é”™è¯¯")
        
        return []
    
    def download_image(self, image_url: str, filename: str) -> bool:
        """ä¸‹è½½å›¾ç‰‡"""
        if not image_url:
            return False
            
        try:
            # æ·»åŠ å›¾ç‰‡å¤„ç†å‚æ•°ä»¥è·å–é«˜è´¨é‡å›¾ç‰‡
            if '?' in image_url:
                image_url = image_url.split('?')[0]
            
            image_url += '?x-oss-process=image/resize,w_1024,m_lfit/format,webp'
            
            response = self.safe_request('GET', image_url, timeout=30)
            if response:
                filepath = os.path.join(self.images_dir, filename)
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                return True
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
        
        return False
    
    def extract_tags_and_keywords(self, model_data: Dict[str, Any]) -> Dict[str, List[str]]:
        """æå–æ ‡ç­¾å’Œå…³é”®è¯"""
        tags = []
        keywords = []
        
        # ä»æ¨¡å‹åç§°æå–å…³é”®è¯
        title = model_data.get('title', '')
        if title:
            # æå–ä¸­æ–‡å…³é”®è¯
            chinese_words = re.findall(r'[\u4e00-\u9fff]+', title)
            keywords.extend(chinese_words)
            
            # æå–è‹±æ–‡å…³é”®è¯
            english_words = re.findall(r'[A-Za-z]+', title)
            keywords.extend(english_words)
        
        # ä»æ ‡ç­¾åˆ—è¡¨æå–
        tag_list = model_data.get('tagList', [])
        for tag in tag_list:
            if isinstance(tag, dict):
                tag_name = tag.get('name', '')
                if tag_name:
                    tags.append(tag_name)
            elif isinstance(tag, str):
                tags.append(tag)
        
        # ä»æè¿°ä¸­æå–å…³é”®è¯
        description = model_data.get('description', '')
        if description:
            desc_keywords = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z]+', description)
            keywords.extend(desc_keywords)
        
        return {
            'tags': list(set(tags)),
            'keywords': list(set(keywords))
        }
    
    def analyze_car_style(self, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ±½è½¦é£æ ¼å’Œç±»å‹"""
        title = model_data.get('title', '').lower()
        description = model_data.get('description', '').lower()
        text = f"{title} {description}"
        
        style_analysis = {
            'vehicle_types': [],
            'design_styles': [],
            'render_styles': [],
            'use_cases': []
        }
        
        # è½¦è¾†ç±»å‹è¯†åˆ«
        vehicle_keywords = {
            'è·‘è½¦': ['è·‘è½¦', 'sports car', 'supercar'],
            'è½¿è½¦': ['è½¿è½¦', 'sedan', 'è½¿è½¦'],
            'SUV': ['suv', 'è¶Šé‡', 'off-road'],
            'å¡è½¦': ['å¡è½¦', 'truck', 'è´§è½¦'],
            'å·´å£«': ['å·´å£«', 'bus', 'å…¬äº¤'],
            'æ‘©æ‰˜è½¦': ['æ‘©æ‰˜', 'motorcycle', 'æœºè½¦'],
            'æ¦‚å¿µè½¦': ['æ¦‚å¿µ', 'concept', 'æœªæ¥'],
            'èµ›è½¦': ['èµ›è½¦', 'racing', 'f1', 'formula'],
            'çš®å¡': ['çš®å¡', 'pickup'],
            'é¢åŒ…è½¦': ['é¢åŒ…è½¦', 'van', 'å•†åŠ¡è½¦']
        }
        
        for vehicle_type, keywords in vehicle_keywords.items():
            if any(keyword in text for keyword in keywords):
                style_analysis['vehicle_types'].append(vehicle_type)
        
        # è®¾è®¡é£æ ¼è¯†åˆ«
        design_keywords = {
            'ç§‘å¹»': ['ç§‘å¹»', 'sci-fi', 'æœªæ¥', 'future'],
            'å¤å¤': ['å¤å¤', 'vintage', 'retro', 'ç»å…¸'],
            'ç°ä»£': ['ç°ä»£', 'modern', 'ç®€çº¦'],
            'è±ªå': ['è±ªå', 'luxury', 'é«˜ç«¯'],
            'è¿åŠ¨': ['è¿åŠ¨', 'sport', 'åŠ¨æ„Ÿ'],
            'å·¥ä¸š': ['å·¥ä¸š', 'industrial', 'æœºæ¢°'],
            'æç®€': ['æç®€', 'minimal', 'ç®€æ´']
        }
        
        for style, keywords in design_keywords.items():
            if any(keyword in text for keyword in keywords):
                style_analysis['design_styles'].append(style)
        
        # æ¸²æŸ“é£æ ¼è¯†åˆ«
        render_keywords = {
            'å†™å®': ['å†™å®', 'realistic', 'çœŸå®'],
            'æ’ç”»': ['æ’ç”»', 'illustration', 'æ‰‹ç»˜'],
            '3Dæ¸²æŸ“': ['3d', 'render', 'æ¸²æŸ“'],
            'æ¦‚å¿µå›¾': ['æ¦‚å¿µ', 'concept', 'è‰å›¾'],
            'æŠ€æœ¯å›¾': ['æŠ€æœ¯', 'technical', 'å·¥ç¨‹å›¾'],
            'æµ·æŠ¥': ['æµ·æŠ¥', 'poster', 'å¹¿å‘Š']
        }
        
        for style, keywords in render_keywords.items():
            if any(keyword in text for keyword in keywords):
                style_analysis['render_styles'].append(style)
        
        # ä½¿ç”¨åœºæ™¯è¯†åˆ«
        use_case_keywords = {
            'æ¸¸æˆ': ['æ¸¸æˆ', 'game', 'æ¸¸æˆè®¾è®¡'],
            'å¹¿å‘Š': ['å¹¿å‘Š', 'advertising', 'è¥é”€'],
            'ç”µå½±': ['ç”µå½±', 'movie', 'å½±è§†'],
            'å·¥ä¸šè®¾è®¡': ['å·¥ä¸šè®¾è®¡', 'industrial design'],
            'æ±½è½¦è®¾è®¡': ['æ±½è½¦è®¾è®¡', 'automotive design'],
            'æ¦‚å¿µè®¾è®¡': ['æ¦‚å¿µè®¾è®¡', 'concept design']
        }
        
        for use_case, keywords in use_case_keywords.items():
            if any(keyword in text for keyword in keywords):
                style_analysis['use_cases'].append(use_case)
        
        return style_analysis
    
    def process_single_model(self, model_basic: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ¨¡å‹çš„å®Œæ•´ä¿¡æ¯"""
        model_uuid = model_basic.get('uuid')
        if not model_uuid or model_uuid in self.collected_models:
            return None
        
        logger.info(f"å¤„ç†æ¨¡å‹: {model_basic.get('title', 'Unknown')} ({model_uuid})")
        
        try:
            # è·å–è¯¦ç»†ä¿¡æ¯
            detail = self.get_model_detail(model_uuid)
            if not detail or detail.get('code') != 0:
                logger.warning(f"æ— æ³•è·å–æ¨¡å‹è¯¦æƒ…: {model_uuid}")
                return None
            
            model_data = detail.get('data', {})
            
            # è·å–ç‰ˆæœ¬ä¿¡æ¯
            versions = self.get_model_versions(model_uuid)
            
            # è·å–ä½œè€…ä¿¡æ¯
            author_info = self.get_model_author(model_uuid)
            author_data = author_info.get('data', {}) if author_info else {}
            
            # è·å–è¯„è®º
            comments = self.get_model_comments(model_uuid)
            
            # è·å–è¿”å›¾
            returns = self.get_model_returns(model_uuid)
            
            # æå–æ ‡ç­¾å’Œå…³é”®è¯
            tags_keywords = self.extract_tags_and_keywords(model_data)
            
            # åˆ†ææ±½è½¦é£æ ¼
            style_analysis = self.analyze_car_style(model_data)
            
            # æ„å»ºå®Œæ•´æ¨¡å‹ä¿¡æ¯
            complete_model = {
                'uuid': model_uuid,
                'title': model_data.get('title', ''),
                'description': model_data.get('description', ''),
                'type': model_data.get('type', ''),
                'baseModel': model_data.get('baseModel', ''),
                'triggerWords': model_data.get('triggerWords', []),
                'nsfw': model_data.get('nsfw', False),
                'allowNoCredit': model_data.get('allowNoCredit', False),
                'allowCommercialUse': model_data.get('allowCommercialUse', False),
                'allowDerivatives': model_data.get('allowDerivatives', False),
                'allowDifferentLicense': model_data.get('allowDifferentLicense', False),
                
                # ç»Ÿè®¡æ•°æ®
                'stats': {
                    'downloadCount': model_data.get('downloadCount', 0),
                    'favoriteCount': model_data.get('favoriteCount', 0),
                    'likeCount': model_data.get('likeCount', 0),
                    'commentCount': model_data.get('commentCount', 0),
                    'generateCount': model_data.get('generateCount', 0),
                    'viewCount': model_data.get('viewCount', 0)
                },
                
                # ä½œè€…ä¿¡æ¯
                'author': {
                    'uuid': author_data.get('uuid', ''),
                    'username': author_data.get('username', ''),
                    'nickname': author_data.get('nickname', ''),
                    'avatar': author_data.get('avatar', ''),
                    'followerCount': author_data.get('followerCount', 0),
                    'modelCount': author_data.get('modelCount', 0)
                },
                
                # ç‰ˆæœ¬ä¿¡æ¯
                'versions': versions,
                
                # æ ‡ç­¾å’Œå…³é”®è¯
                'tags': tags_keywords['tags'],
                'keywords': tags_keywords['keywords'],
                
                # æ±½è½¦é£æ ¼åˆ†æ
                'car_analysis': style_analysis,
                
                # è¯„è®ºæ‘˜è¦
                'comments_summary': {
                    'total_comments': len(comments),
                    'recent_comments': comments[:5]  # æœ€è¿‘5æ¡è¯„è®º
                },
                
                # è¿”å›¾æ‘˜è¦
                'returns_summary': {
                    'total_returns': len(returns),
                    'recent_returns': returns[:10]  # æœ€è¿‘10ä¸ªè¿”å›¾
                },
                
                # å›¾ç‰‡ä¿¡æ¯
                'images': [],
                
                # é‡‡é›†æ—¶é—´
                'collected_at': datetime.now(timezone.utc).isoformat(),
                'collection_timestamp': self.get_timestamp()
            }
            
            # å¤„ç†å›¾ç‰‡
            images_to_download = []
            
            # ä¸»é¢„è§ˆå›¾
            if model_data.get('images'):
                for i, img in enumerate(model_data['images'][:5]):  # æœ€å¤š5å¼ ä¸»å›¾
                    img_url = img.get('url', '')
                    if img_url:
                        filename = f"{model_uuid}_main_{i+1}.webp"
                        images_to_download.append((img_url, filename))
                        complete_model['images'].append({
                            'type': 'main',
                            'url': img_url,
                            'filename': filename,
                            'width': img.get('width', 0),
                            'height': img.get('height', 0)
                        })
            
            # ç‰ˆæœ¬å›¾ç‰‡
            for version in versions:
                version_images = version.get('images', [])
                for i, img in enumerate(version_images[:3]):  # æ¯ä¸ªç‰ˆæœ¬æœ€å¤š3å¼ å›¾
                    img_url = img.get('url', '')
                    if img_url:
                        filename = f"{model_uuid}_version_{version.get('uuid', '')}_{i+1}.webp"
                        images_to_download.append((img_url, filename))
                        complete_model['images'].append({
                            'type': 'version',
                            'version_uuid': version.get('uuid', ''),
                            'url': img_url,
                            'filename': filename,
                            'width': img.get('width', 0),
                            'height': img.get('height', 0)
                        })
            
            # è¿”å›¾ç¤ºä¾‹
            for i, return_item in enumerate(returns[:5]):  # æœ€å¤š5ä¸ªè¿”å›¾ç¤ºä¾‹
                img_url = return_item.get('url', '')
                if img_url:
                    filename = f"{model_uuid}_return_{i+1}.webp"
                    images_to_download.append((img_url, filename))
                    complete_model['images'].append({
                        'type': 'return',
                        'url': img_url,
                        'filename': filename,
                        'prompt': return_item.get('prompt', ''),
                        'user': return_item.get('user', {})
                    })
            
            # æ‰¹é‡ä¸‹è½½å›¾ç‰‡
            downloaded_count = 0
            with ThreadPoolExecutor(max_workers=5) as executor:
                download_futures = {
                    executor.submit(self.download_image, url, filename): (url, filename) 
                    for url, filename in images_to_download
                }
                
                for future in as_completed(download_futures):
                    url, filename = download_futures[future]
                    try:
                        if future.result():
                            downloaded_count += 1
                    except Exception as e:
                        logger.error(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸ {filename}: {e}")
            
            complete_model['download_stats'] = {
                'total_images': len(images_to_download),
                'downloaded_images': downloaded_count
            }
            
            logger.info(f"æ¨¡å‹å¤„ç†å®Œæˆ: {complete_model['title']} (å›¾ç‰‡: {downloaded_count}/{len(images_to_download)})")
            
            self.collected_models.add(model_uuid)
            return complete_model
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¨¡å‹å¼‚å¸¸ {model_uuid}: {e}")
            return None
    
    def collect_all_car_models(self) -> List[Dict[str, Any]]:
        """é‡‡é›†æ‰€æœ‰æ±½è½¦äº¤é€šæ¨¡å‹"""
        logger.info("å¼€å§‹é‡‡é›†æ±½è½¦äº¤é€šæ¿å—æ‰€æœ‰æ¨¡å‹...")
        
        page = 1
        total_collected = 0
        
        while True:
            # è·å–æ¨¡å‹åˆ—è¡¨
            models_response = self.get_car_models_list(page=page)
            
            if not models_response or models_response.get('code') != 0:
                logger.error(f"è·å–ç¬¬ {page} é¡µæ¨¡å‹åˆ—è¡¨å¤±è´¥")
                break
            
            data = models_response.get('data', {})
            models_list = data.get('list', [])
            
            if not models_list:
                logger.info(f"ç¬¬ {page} é¡µæ— æ¨¡å‹ï¼Œé‡‡é›†å®Œæˆ")
                break
            
            logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(models_list)} ä¸ªæ¨¡å‹")
            
            # å¹¶è¡Œå¤„ç†æ¨¡å‹
            with ThreadPoolExecutor(max_workers=3) as executor:
                future_to_model = {
                    executor.submit(self.process_single_model, model): model 
                    for model in models_list
                }
                
                for future in as_completed(future_to_model):
                    model_basic = future_to_model[future]
                    try:
                        result = future.result()
                        if result:
                            self.models_data.append(result)
                            total_collected += 1
                            
                            # æ¯å¤„ç†10ä¸ªæ¨¡å‹ä¿å­˜ä¸€æ¬¡
                            if total_collected % 10 == 0:
                                self.save_progress()
                                
                    except Exception as e:
                        logger.error(f"å¤„ç†æ¨¡å‹å¼‚å¸¸: {e}")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
            total_count = data.get('totalCount', 0)
            current_count = page * 24
            
            if current_count >= total_count:
                logger.info(f"å·²é‡‡é›†å®Œæ‰€æœ‰æ¨¡å‹ï¼Œæ€»è®¡: {total_collected}")
                break
            
            page += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        logger.info(f"æ±½è½¦äº¤é€šæ¨¡å‹é‡‡é›†å®Œæˆï¼Œå…±é‡‡é›† {total_collected} ä¸ªæ¨¡å‹")
        return self.models_data
    
    def save_progress(self):
        """ä¿å­˜é‡‡é›†è¿›åº¦"""
        progress_file = os.path.join(self.output_dir, 'collection_progress.json')
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump({
                'collected_count': len(self.models_data),
                'collected_models': list(self.collected_models),
                'last_update': datetime.now(timezone.utc).isoformat()
            }, f, ensure_ascii=False, indent=2)
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ä¿å­˜å®Œæ•´æ•°æ®
        output_file = os.path.join(self.output_dir, 'complete_car_models_data.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.models_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é‡‡é›†ç»Ÿè®¡
        stats = self.generate_collection_stats()
        stats_file = os.path.join(self.output_dir, 'collection_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {self.output_dir}/")
    
    def generate_collection_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆé‡‡é›†ç»Ÿè®¡"""
        if not self.models_data:
            return {}
        
        stats = {
            'collection_summary': {
                'total_models': len(self.models_data),
                'collection_time': datetime.now(timezone.utc).isoformat(),
                'total_images': sum(len(model.get('images', [])) for model in self.models_data),
                'total_downloads': sum(model.get('download_stats', {}).get('downloaded_images', 0) for model in self.models_data)
            },
            'model_types': {},
            'base_models': {},
            'authors': {},
            'vehicle_types': {},
            'design_styles': {},
            'render_styles': {},
            'use_cases': {},
            'top_models': []
        }
        
        # ç»Ÿè®¡æ¨¡å‹ç±»å‹
        for model in self.models_data:
            model_type = model.get('type', 'Unknown')
            stats['model_types'][model_type] = stats['model_types'].get(model_type, 0) + 1
        
        # ç»Ÿè®¡åŸºç¡€æ¨¡å‹
        for model in self.models_data:
            base_model = model.get('baseModel', 'Unknown')
            stats['base_models'][base_model] = stats['base_models'].get(base_model, 0) + 1
        
        # ç»Ÿè®¡ä½œè€…
        for model in self.models_data:
            author = model.get('author', {}).get('username', 'Unknown')
            if author not in stats['authors']:
                stats['authors'][author] = {
                    'model_count': 0,
                    'total_likes': 0,
                    'total_downloads': 0
                }
            stats['authors'][author]['model_count'] += 1
            stats['authors'][author]['total_likes'] += model.get('stats', {}).get('likeCount', 0)
            stats['authors'][author]['total_downloads'] += model.get('stats', {}).get('downloadCount', 0)
        
        # ç»Ÿè®¡æ±½è½¦åˆ†ææ•°æ®
        for model in self.models_data:
            car_analysis = model.get('car_analysis', {})
            
            for vehicle_type in car_analysis.get('vehicle_types', []):
                stats['vehicle_types'][vehicle_type] = stats['vehicle_types'].get(vehicle_type, 0) + 1
            
            for design_style in car_analysis.get('design_styles', []):
                stats['design_styles'][design_style] = stats['design_styles'].get(design_style, 0) + 1
            
            for render_style in car_analysis.get('render_styles', []):
                stats['render_styles'][render_style] = stats['render_styles'].get(render_style, 0) + 1
            
            for use_case in car_analysis.get('use_cases', []):
                stats['use_cases'][use_case] = stats['use_cases'].get(use_case, 0) + 1
        
        # è·å–çƒ­é—¨æ¨¡å‹ï¼ˆæŒ‰ç‚¹èµæ•°æ’åºï¼‰
        sorted_models = sorted(
            self.models_data, 
            key=lambda x: x.get('stats', {}).get('likeCount', 0), 
            reverse=True
        )
        
        stats['top_models'] = [
            {
                'title': model.get('title', ''),
                'uuid': model.get('uuid', ''),
                'author': model.get('author', {}).get('username', ''),
                'likes': model.get('stats', {}).get('likeCount', 0),
                'downloads': model.get('stats', {}).get('downloadCount', 0),
                'generates': model.get('stats', {}).get('generateCount', 0)
            }
            for model in sorted_models[:20]
        ]
        
        return stats

class CompleteCarModelScraper(LiblibCarModelsScraper):
    def load_config(self) -> Dict[str, Any]:
        """Return a minimal configuration dict expected by tests.
        This keeps backward compatibility with older test imports.
        """
        return {
            'api_base': self.api_base,
            'output_dir': self.output_dir,
            'images_dir': self.images_dir,
            'default_page_size': 24,
            'default_concurrency': 3,
        }

def main():
    """ä¸»å‡½æ•°"""
    scraper = LiblibCarModelsScraper()
    
    try:
        # é‡‡é›†æ‰€æœ‰æ±½è½¦äº¤é€šæ¨¡å‹
        models = scraper.collect_all_car_models()
        
        # ä¿å­˜ç»“æœ
        scraper.save_final_results()
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡é‡‡é›† {len(models)} ä¸ªæ±½è½¦äº¤é€šæ¨¡å‹")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ {scraper.output_dir}/ ç›®å½•ä¸­")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿å­˜åœ¨ {scraper.images_dir}/ ç›®å½•ä¸­")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
        scraper.save_progress()
        print(f"ğŸ“ å·²ä¿å­˜å½“å‰è¿›åº¦åˆ° {scraper.output_dir}/")
    except Exception as e:
        logger.error(f"é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        scraper.save_progress()

if __name__ == "__main__":
    main()
