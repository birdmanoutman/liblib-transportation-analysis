#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pytesté…ç½®æ–‡ä»¶
è®¾ç½®æµ‹è¯•ç¯å¢ƒå’Œè·¯å¾„
"""

import os
import sys
import pytest
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.setdefault('TESTING', 'true')
os.environ.setdefault('DB_HOST', 'localhost')
os.environ.setdefault('DB_PORT', '3306')
os.environ.setdefault('DB_NAME', 'test_cardesignspace')
os.environ.setdefault('DB_USER', 'test_user')
os.environ.setdefault('DB_PASSWORD', 'test_password')

# æµ‹è¯•é…ç½®
def pytest_configure(config):
    """pytesté…ç½®é’©å­"""
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "integration: marks tests as integration tests"
    )
    config.addinivalue_line(
        "markers", "unit: marks tests as unit tests"
    )
    config.addinivalue_line(
        "markers", "performance: marks tests as performance tests"
    )
    config.addinivalue_line(
        "markers", "api: marks tests as API tests"
    )
    config.addinivalue_line(
        "markers", "scraping: marks tests as scraping tests"
    )
    config.addinivalue_line(
        "markers", "database: marks tests as database tests"
    )
    config.addinivalue_line(
        "markers", "analysis: marks tests as analysis tests"
    )

def pytest_collection_modifyitems(config, items):
    """ä¿®æ”¹æµ‹è¯•é¡¹é›†åˆï¼Œè‡ªåŠ¨æ·»åŠ æ ‡è®°"""
    for item in items:
        # æ ¹æ®æ–‡ä»¶è·¯å¾„è‡ªåŠ¨æ·»åŠ æ ‡è®°
        file_path = str(item.fspath)
        
        if "unit" in file_path:
            item.add_marker("unit")
        elif "integration" in file_path:
            item.add_marker("integration")
        
        # æ ¹æ®æ–‡ä»¶åæ·»åŠ ç‰¹å®šæ ‡è®°
        if "test_performance" in file_path:
            item.add_marker("performance")
        elif "test_api" in file_path:
            item.add_marker("api")
        elif "test_data_collection" in file_path:
            item.add_marker("scraping")
        elif "test_database" in file_path or "database" in file_path:
            item.add_marker("database")
        elif "test_analysis" in file_path or "analysis" in file_path:
            item.add_marker("analysis")
        
        # æ ¹æ®æµ‹è¯•å‡½æ•°åæ·»åŠ æ ‡è®°
        if "performance" in item.name:
            item.add_marker("performance")
        elif "api" in item.name:
            item.add_marker("api")
        elif "scraping" in item.name or "collector" in item.name:
            item.add_marker("scraping")
        elif "database" in item.name or "db" in item.name:
            item.add_marker("database")
        elif "analysis" in item.name:
            item.add_marker("analysis")

# æµ‹è¯•å¤¹å…·
@pytest.fixture(scope="session")
def project_root_path():
    """è¿”å›é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"""
    return project_root

@pytest.fixture(scope="session")
def src_path():
    """è¿”å›srcç›®å½•è·¯å¾„"""
    return project_root / "src"

@pytest.fixture(scope="session")
def test_data_path():
    """è¿”å›æµ‹è¯•æ•°æ®ç›®å½•è·¯å¾„"""
    return project_root / "tests" / "fixtures"

@pytest.fixture(scope="session")
def output_path():
    """è¿”å›è¾“å‡ºç›®å½•è·¯å¾„"""
    output_dir = project_root / "test_output"
    output_dir.mkdir(exist_ok=True)
    return output_dir

@pytest.fixture(scope="function")
def temp_file():
    """åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹å…·"""
    import tempfile
    
    temp_file = tempfile.NamedTemporaryFile(delete=False)
    temp_file.close()
    
    yield temp_file.name
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try:
        os.unlink(temp_file.name)
    except OSError:
        pass

@pytest.fixture(scope="function")
def temp_dir():
    """åˆ›å»ºä¸´æ—¶ç›®å½•å¤¹å…·"""
    import tempfile
    import shutil
    
    temp_dir = tempfile.mkdtemp()
    
    yield temp_dir
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    try:
        shutil.rmtree(temp_dir)
    except OSError:
        pass

@pytest.fixture(scope="session")
def test_config():
    """è¿”å›æµ‹è¯•é…ç½®"""
    return {
        "api_base": "https://api2.liblib.art",
        "max_workers": 2,
        "timeout": 10,
        "retry_attempts": 2,
        "retry_delay": 0.1,
        "rate_limit": {
            "max_requests": 10,
            "time_window": 60
        },
        "database": {
            "host": "localhost",
            "port": 3306,
            "name": "test_cardesignspace",
            "user": "test_user",
            "password": "test_password"
        },
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(levelname)s - %(message)s"
        }
    }

@pytest.fixture(scope="session")
def sample_car_models():
    """è¿”å›ç¤ºä¾‹æ±½è½¦æ¨¡å‹æ•°æ®"""
    from tests.fixtures.test_data import SAMPLE_CAR_MODELS
    return SAMPLE_CAR_MODELS

@pytest.fixture(scope="session")
def sample_api_response():
    """è¿”å›ç¤ºä¾‹APIå“åº”æ•°æ®"""
    from tests.fixtures.test_data import SAMPLE_API_RESPONSE
    return SAMPLE_API_RESPONSE

@pytest.fixture(scope="session")
def mock_session():
    """è¿”å›æ¨¡æ‹Ÿä¼šè¯å¯¹è±¡"""
    from tests.fixtures.test_data import create_mock_session
    return create_mock_session()

@pytest.fixture(scope="session")
def mock_response():
    """è¿”å›æ¨¡æ‹Ÿå“åº”å¯¹è±¡"""
    from tests.fixtures.test_data import create_mock_response
    return create_mock_response

# ç¯å¢ƒæ£€æŸ¥
def pytest_sessionstart(session):
    """æµ‹è¯•ä¼šè¯å¼€å§‹æ—¶çš„æ£€æŸ¥"""
    print(f"ğŸ” æµ‹è¯•ç¯å¢ƒæ£€æŸ¥:")
    print(f"   é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"   Pythonç‰ˆæœ¬: {sys.version}")
    print(f"   å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ£€æŸ¥å¿…è¦çš„ç›®å½•
    required_dirs = ["src", "tests", "config"]
    for dir_name in required_dirs:
        dir_path = project_root / dir_name
        if dir_path.exists():
            print(f"   âœ… {dir_name}: {dir_path}")
        else:
            print(f"   âŒ {dir_name}: ä¸å­˜åœ¨")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    env_vars = ["TESTING", "DB_HOST", "DB_NAME"]
    for var in env_vars:
        value = os.environ.get(var, "æœªè®¾ç½®")
        print(f"   ğŸ“‹ {var}: {value}")

def pytest_sessionfinish(session, exitstatus):
    """æµ‹è¯•ä¼šè¯ç»“æŸæ—¶çš„æ¸…ç†"""
    print(f"\nğŸ§¹ æµ‹è¯•ä¼šè¯æ¸…ç†:")
    print(f"   é€€å‡ºçŠ¶æ€: {exitstatus}")
    print(f"   æµ‹è¯•æ”¶é›†: {len(session.items)} ä¸ªæµ‹è¯•é¡¹")
    
    # æ¸…ç†æµ‹è¯•è¾“å‡ºç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶
    test_output_dir = project_root / "test_output"
    if test_output_dir.exists():
        temp_files = list(test_output_dir.glob("temp_*"))
        for temp_file in temp_files:
            try:
                temp_file.unlink()
                print(f"   ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file.name}")
            except OSError:
                pass

# è‡ªå®šä¹‰pytesté€‰é¡¹
def pytest_addoption(parser):
    """æ·»åŠ è‡ªå®šä¹‰pytesté€‰é¡¹"""
    parser.addoption(
        "--run-slow",
        action="store_true",
        default=False,
        help="è¿è¡Œæ ‡è®°ä¸ºslowçš„æµ‹è¯•"
    )
    
    parser.addoption(
        "--run-performance",
        action="store_true",
        default=False,
        help="è¿è¡Œæ€§èƒ½æµ‹è¯•"
    )
    
    parser.addoption(
        "--run-integration",
        action="store_true",
        default=False,
        help="è¿è¡Œé›†æˆæµ‹è¯•"
    )

def pytest_runtest_setup(item):
    """æµ‹è¯•è¿è¡Œå‰çš„è®¾ç½®"""
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ…¢é€Ÿæµ‹è¯•
    if not item.config.getoption("--run-slow") and "slow" in item.keywords:
        pytest.skip("éœ€è¦ --run-slow é€‰é¡¹æ¥è¿è¡Œæ…¢é€Ÿæµ‹è¯•")
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ€§èƒ½æµ‹è¯•
    if not item.config.getoption("--run-performance") and "performance" in item.keywords:
        pytest.skip("éœ€è¦ --run-performance é€‰é¡¹æ¥è¿è¡Œæ€§èƒ½æµ‹è¯•")
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡é›†æˆæµ‹è¯•
    if not item.config.getoption("--run-integration") and "integration" in item.keywords:
        pytest.skip("éœ€è¦ --run-integration é€‰é¡¹æ¥è¿è¡Œé›†æˆæµ‹è¯•")
